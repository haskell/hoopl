\iffalse

Payload: alternatives for functions polymorphic in node type

  (n C O -> a, n C O -> b, n C O -> c)

vs

  (forall e x . n e x -> ShapeTag e x, forall e x . n e x -> result e x)

where 

  result C O = a
  result O O = b
  result O C = c

The function returning ShapeTag can be passed in a type-class
dictionary. 




Say something about the proliferation of heavyweight type signatures
required for GADT pattern matches.  When the signature is three times
the size of the function, something is wrong...


I'm going to leave this point out, because in all the client code
we've written, we are (a) matching only on a node, and (b) matching on
all cases.  In this scenario the GADT exhaustiveness checker provides
no additional benefits.  Indeed, GADTs can be an irritant to the
client: in many pattern matches, GADTs make it impossible to default
cases.


I was thinking again about unwrapped nodes, cons-lists, snoc-lists,
and tree fringe.  I think there's an analogy with ordinary lists, and
the analogy is 'concatMap' vs 'fold'.  Just as with lists, the common
case of 'concatMap (\a -> [a])' does a lot of unnecessary wrapping and
unwrapping of elements.  We nevertheless prefer 'concatMap' because it
provides better separation of concerns.  But this analogy suggests
several ideas:

  - Can block processing be written in higher-order fashion?

  - Can it be done in both styles (concatMap *and* fold)?

  - Once we have framed the problem in these terms, can we write
    fold-style cold that is not too hard to understand and maintain?

  - Finally, is there an analog of stream fusion that would work for
    control-flow graphs, enabling us to write some graph combinators
    that be both perspicuous and efficient?

These are good observations for the paper and for future work.


----------------------------------------------------------------


P.S. The three of us should have a nice little Skype chat about
higher-rank types.  A lot of the questions still at issue boil down to
the following observations:

  - It's really convenient for the *implementation* of Hoopl to put
    forall to the left of an arrow.  Polymorphic functions as
    arguments are powerful and keen, and they make it really easy for
    Hoopl to do its job.

  - It's a bid inconvenient for a *client* of Hoopl to be forced to
    supply functions that are polymorphic in shape.  All constructors
    have to be written out by hand; defaulting is not possible.  (Or
    at least John and I haven't figured out how to do it.)

  - Using higher-rank types in Hoopl's interface makes some very
    desirable things downright impossible:

      - One can't write a generic dominator analysis with type

           dom :: Edges n => FwdPass n Dominators

      - One can't write a generic debugging wrapper of type

           debug :: (Show n, Show f)
                 => TraceFn -> WhatToTrace -> FwdPass n f -> FwdPass n f

      - One can't write a cominator of type

           product :: FwdPass n f -> FwdPass n f' -> FwdPass n (f, f')

    I submit that these things are all very desirable.

I'm therefore strongly in favor of removing the *requirement* that a
FwdPass include transfer and rewrite functions that are polymorphic in
shape.  It will be a bit tedious to return to triples of functions,
but for those lucky clients who *can* write polymorphic functions and
who wish to, we can provide injection functions.

I should stress that I believe the tedium can be contained within
reasonable bounds---for example, the arfXXX functions that are
internal to Hoopl all remain polymorphic in shape (although not
higher-rank any longer).

\fi


%
% TODO
%
%
% AGraph = graph under construction
% Graph = replacement graph
%
%%%  Hoopl assumes that the replacement graph for a node open at the exit
%%%  doesn't contain any additional exits
%%%  
%%%  introduce replacement graph in section 3, after graph.
%%%  it has arbitrarily complicated internal control flow, but only
%%%  one exit (if open at the exit)
%%%  
%%%  a rquirement on the client that is not checked statically.


\input{dfoptdu.tex}

\newif\ifpagetuning \pagetuningtrue  % adjust page breaks

\newif\ifnoauthornotes \noauthornotesfalse

\newif\iftimestamp\timestamptrue  % show MD5 stamp of paper

% \timestampfalse % it's post-submission time

\IfFileExists{timestamp.tex}{}{\timestampfalse}

\newif\ifcutting \cuttingfalse % cutting down to submission size


\newif\ifgenkill\genkillfalse  % have a section on gen and kill
\genkillfalse


\newif\ifnotesinmargin \notesinmarginfalse
\IfFileExists{notesinmargin.tex}{\notesinmargintrue}{\relax}

\documentclass[blockstyle,preprint,natbib,nocopyrightspace]{sigplanconf}

\newcommand\ourlib{Hoopl}
   % higher-order optimization library
   % ('Hoople' was taken -- see hoople.org)
\let\hoopl\ourlib

\newcommand\ag{\ensuremath{\mathit{ag}}}
\renewcommand\ag{\ensuremath{g}}  % not really seeing that 'ag' is helpful here ---NR
\newcommand\rw{\ensuremath{\mathit{rw}}}

% l2h substitution ourlib Hoopl
% l2h substitution hoopl Hoopl

\newcommand\fs{\ensuremath{\mathit{fs}}} % dataflow facts, possibly plural

\newcommand\vfilbreak[1][\baselineskip]{%
  \vskip 0pt plus #1 \penalty -200 \vskip 0pt plus -#1 }

\usepackage{alltt}
\usepackage{array}
\usepackage{afterpage}
\newcommand\lbr{\char`\{}
\newcommand\rbr{\char`\}}
 
\clubpenalty=10000
\widowpenalty=10000

\usepackage{verbatim} % allows to define \begin{smallcode}
\newenvironment{smallcode}{\par\unskip\small\verbatim}{\endverbatim}
\newenvironment{fuzzcode}[1]{\par\unskip\hfuzz=#1 \verbatim}{\endverbatim}
\newenvironment{smallfuzzcode}[1]{\par\unskip\small\hfuzz=#1 \verbatim}{\endverbatim}

\newcommand\smallverbatiminput[1]{{\small\verbatiminput{#1}}}

\newcommand\lineref[1]{line~\ref{line:#1}}
\newcommand\linepairref[2]{lines \ref{line:#1}~and~\ref{line:#2}}
\newcommand\linerangeref[2]{\mbox{lines~\ref{line:#1}--\ref{line:#2}}}
\newcommand\Lineref[1]{Line~\ref{line:#1}}
\newcommand\Linepairref[2]{Lines \ref{line:#1}~and~\ref{line:#2}}
\newcommand\Linerangeref[2]{\mbox{Lines~\ref{line:#1}--\ref{line:#2}}}

\makeatletter

\let\c@table=
           \c@figure % one counter for tables and figures, please

\newcommand\setlabel[1]{%
  \setlabel@#1!!\@endsetlabel
}
\def\setlabel@#1!#2!#3\@endsetlabel{%
  \ifx*#1*% line begins with label or is empty
     \ifx*#2*% line is empty
        \verbatim@line{}%
     \else
       \@stripbangs#3\@endsetlabel%
       \label{line:#2}%
     \fi
  \else
     \@stripbangs#1!#2!#3\@endsetlabel%
  \fi
}
\def\@stripbangs#1!!\@endsetlabel{%
  \verbatim@line{#1}%
}


\verbatim@line{hello mama}

\newcommand{\numberedcodebackspace}{0.5\baselineskip}

\newcounter{codeline}
\newenvironment{numberedcode}
  {\endgraf
     \def\verbatim@processline{%
        \noindent
        \expandafter\ifx\expandafter+\the\verbatim@line+  % blank line
               %{\small\textit{\def\rmdefault{cmr}\rmfamily\phantom{00}\phantom{: \,}}}%
            \else
               \refstepcounter{codeline}%
               {\small\textit{\def\rmdefault{cmr}\rmfamily\phantom{00}\llap{\arabic{codeline}}: \,}}%
            \fi
        \expandafter\setlabel\expandafter{\the\verbatim@line}%
        \expandafter\ifx\expandafter+\the\verbatim@line+  % blank line
          \vspace*{-\numberedcodebackspace}\par%
        \else
          \the\verbatim@line\par
        \fi}%
   \verbatim
   }
   {\endverbatim}

\makeatother

\newcommand\arrow{\rightarrow}

\newcommand\join{\sqcup}
\newcommand\slotof[1]{\ensuremath{s_{#1}}}
\newcommand\tempof[1]{\ensuremath{t_{#1}}}
\let\tempOf=\tempof
\let\slotOf=\slotof

\makeatletter
\newcommand{\nrmono}[1]{%
  {\@tempdima = \fontdimen2\font\relax
   \texttt{\spaceskip = 1.1\@tempdima #1}}}
\makeatother

\usepackage{times}  % denser fonts
\renewcommand{\ttdefault}{aett} % \texttt that goes better with times fonts
\usepackage{enumerate}
\usepackage{url}
\usepackage{graphicx}
\usepackage{natbib}  % redundant for Simon
\bibpunct();A{},
\let\cite\citep
\let\citeyearnopar=\citeyear
\let\citeyear=\citeyearpar

\usepackage[ps2pdf,bookmarksopen,breaklinks,pdftitle=dataflow-made-simple]{hyperref}
\usepackage{breakurl} % enables \burl

\newcommand\naive{na\"\i ve}
\newcommand\naively{na\"\i vely}
\newcommand\Naive{Na\"\i ve}

\usepackage{amsfonts}
\newcommand\naturals{\ensuremath{\mathbb{N}}}
\newcommand\true{\ensuremath{\mathbf{true}}}
\newcommand\implies{\supseteq}  % could use \Rightarrow?

\newcommand\PAL{\mbox{C{\texttt{-{}-}}}}
\newcommand\high[1]{\mbox{\fboxsep=1pt \smash{\fbox{\vrule height 6pt
   depth 0pt width 0pt \leavevmode \kern 1pt #1}}}}

\usepackage{tabularx}

%%
%% 2009/05/10: removed 'float' package because it breaks multiple
%% \caption's per {figure} environment.   ---NR
%%
%%  % Put figures in boxes --- WHY??? --NR
%%  \usepackage{float}
%%  \floatstyle{boxed}
%%  \restylefloat{figure}
%%  \restylefloat{table}



% ON LINE THREE, set \noauthornotestrue to suppress notes (or not)

%\newcommand{\qed}{QED}
\ifnotesinmargin
  \long\def\authornote#1{%
      \ifvmode
         \marginpar{\raggedright\hbadness=10000
         \parindent=8pt \parskip=2pt
         \def\baselinestretch{0.8}\tiny
         \itshape\noindent #1\par}%
      \else
          \unskip\raisebox{-3.5pt}{\rlap{$\scriptstyle\diamond$}}%
          \marginpar{\raggedright\hbadness=10000
         \parindent=8pt \parskip=2pt
         \def\baselinestretch{0.8}\tiny
         \itshape\noindent #1\par}%
      \fi}
\else
  % Simon: please set \notesinmarginfalse on the first line
  \long\def\authornote#1{{\em #1\/}}
\fi
\ifnoauthornotes
  \def\authornote#1{\unskip\relax}
\fi

\newcommand{\simon}[1]{\authornote{SLPJ: #1}}
\newcommand{\norman}[1]{\authornote{NR: #1}}
\let\remark\norman
\def\finalremark#1{\relax}
% \let \finalremark \remark % uncomment after submission
\newcommand{\john}[1]{\authornote{JD: #1}}
\newcommand{\todo}[1]{\textbf{To~do:} \emph{#1}}
\newcommand\delendum[1]{\relax\ifvmode\else\unskip\fi\relax}

\newcommand\secref[1]{Section~\ref{sec:#1}}
\newcommand\secreftwo[2]{Sections \ref{sec:#1}~and~\ref{sec:#2}}
\newcommand\seclabel[1]{\label{sec:#1}}

\newcommand\figref[1]{Figure~\ref{fig:#1}}
\newcommand\figreftwo[2]{Figures \ref{fig:#1}~and~\ref{fig:#2}}
\newcommand\figlabel[1]{\label{fig:#1}}

\newcommand\tabref[1]{Table~\ref{tab:#1}}
\newcommand\tablabel[1]{\label{tab:#1}}


\newcommand{\CPS}{\textbf{StkMan}}    % Not sure what to call it.


\usepackage{code}   % At-sign notation

\iftimestamp
\input{timestamp}
\preprintfooter{\mdfivestamp}
\fi

\hyphenation{there-by}

\renewcommand{\floatpagefraction}{0.9} % must be less than \topfraction
\renewcommand{\topfraction}{0.95}
\renewcommand{\textfraction}{0.05}

\begin{document}
%\title{\ourlib: Dataflow Optimization Made Simple}
\title{\ourlib: A Modular, Reusable Library for\\ Dataflow Analysis and Transformation}
%\title{Implementing Dataflow Analysis and Optimization by Lifting Node Functions to Basic Blocks and Control-Flow Graphs}
%\subtitle{Programming pearl}

%%  \titlebanner{\textsf{\mdseries\itshape
%%  Under consideration for publication in the ACM Haskell Symposium.
%%  Comments
%%  are welcome; please identify this version by the words
%%  \textbf{\mdfivewords}.
%%  }}

\ifnoauthornotes
\makeatletter
\let\HyPsd@Warning=
                \@gobble
\makeatother
\fi

% João


\authorinfo{Norman Ramsey}{Tufts University}{nr@cs.tufts.edu}
\authorinfo{Jo\~ao Dias}{Tufts University}{dias@cs.tufts.edu}
\authorinfo{Simon Peyton Jones}{Microsoft Research}{simonpj@microsoft.com}


\maketitle
 
\begin{abstract}
\iffalse % A vote for Simon's abstract
\remark{I have replaced a good abstract of the POPL submission with a
bad abstract of \emph{this} submission.}
We present \ourlib, a Haskell library that makes it easy for a
compiler writer
to implement program transformations based on dataflow analyses.
A~client of \ourlib\ defines a representation of 
logical assertions,
a transfer function that computes outgoing assertions from incoming
assertions, 
and a rewrite function that improves code when improvements are
justified by the assertions.
\ourlib\ does the actual analysis and transformation.

\ourlib\ implements state-of-the art algorithms:
Lerner, Grove, and Chambers's 
\citeyearpar{lerner-grove-chambers:2002}
composition of simple analyses and transformations, which achieves
the same precision as complex, handwritten
``super-analyses;''
and Whalley's \citeyearpar{whalley:isolation} dynamic technique for
isolating bugs in a client's code.
\ourlib's implementation is unique in that unlike previous
implementations,
it carefully separates the tricky
elements of each of these algorithms, so that they can be examined and
understood independently.


\simon{Here is an alternative abstract based on the four-sentence model.}
\remark{Four-sentence model?  You must teach me\ldots}
\fi
Dataflow analysis and transformation of control-flow graphs is
pervasive in optimizing compilers, but it is typically tightly
interwoven with the details of a \emph{particular} compiler.  
We~describe \ourlib{}, a reusable Haskell library that makes it
unusually easy to define new analyses and
transformations for \emph{any} compiler.
\ourlib's interface is modular and polymorphic,
and it offers unusually strong static guarantees.
The implementation
is also far from routine: it encapsulates 
state-of-the-art algorithms (interleaved analysis and rewriting,
dynamic error isolation), and it cleanly separates their tricky elements
so that they can be understood independently.
%
%\ourlib\ will be the workhorse of a new
%back end for the Glasgow Haskell Compiler (version~6.14, forthcoming).

%% \emph{Reviewers:} code examples are indexed at {\small\url{http://bit.ly/jkr3K}}
%%% Source: http://www.cs.tufts.edu/~nr/drop/popl-index.pdf
\end{abstract}

\makeatactive   %  Enable @foo@ notation

\section{Introduction}

A mature optimizing compiler for an imperative language includes many
analyses, the results of which justify the optimizer's
code-improving transformations.
Many of the most important analyses and transformations---constant
propagation, live-variable analysis, inlining, sinking of loads, 
and so on---should be regarded as particular cases of
a single general problem: \emph{dataflow analysis and optimization}.
%% \remark{I do not feel compelled to cite Muchnick (or anyone else) here}
Dataflow analysis is over thirty years old,
but a recent, seminal paper by \citet{lerner-grove-chambers:2002} goes further, 
describing a powerful but subtle way to
\emph{interleave} analysis and transformation so that each 
piggybacks on the other.

Because optimizations based on dataflow analysis 
share a common intellectual framework, and because that framework is
subtle, it it tempting to
try to build a single reusable library that embodies the 
subtle ideas, while
making it easy for clients to instantiate the library for different
situations.
Tempting, but difficult.
Although some such frameworks exist, as we discuss 
in \secref{related}, they have complex APIs and implementations,
and none implements the Lerner/Grove/Chambers technique.

In this paper we present \ourlib{} (short for ``higher-order
optimization library''), a new Haskell library for dataflow analysis and
optimization.  It has the following distinctive characteristics:

\begin{itemize}
\item
\ourlib\ is purely functional.  
Perhaps surprisingly, code that
manipulates control-flow graphs is easier to write, and far easier
to write correctly, when written in a purely functional style
\cite{ramsey-dias:applicative-flow-graph}.
When analysis and rewriting
are interleaved, so that rewriting must be done \emph{speculatively},
without knowing whether
the result of the rewrite will be retained or discarded,
the benefit of a purely functional style is intensified
(Sections \ref{sec:overview} and \ref{sec:fixpoints}).

\item
\ourlib\ is polymorphic. Just as a list library is
polymorphic in the list elements, so is \ourlib{} polymorphic, both in
the nodes that inhabit graphs, and in the dataflow facts that 
analyses compute over these graphs (\secref{using-hoopl}).

\item The paper by Lerner, Grove, and Chambers is inspiring but abstract.
We articulate their ideas in a concrete but simple API that hides 
a subtle implementation (Sections \ref{sec:graph-rep} and \ref{sec:using-hoopl}).  
You provide a representation for assertions, 
a transfer function that transforms assertions across a node, 
and a rewrite function that uses a assertion to 
justify rewriting a node.
\ourlib\ ``lifts'' these node-level functions to work over
control-flow graphs, sets up and solves recursion equations,
and interleaves rewriting with analysis.
Designing good abstractions (data types, APIs) is surprisingly
hard; we have been through over a dozen significantly different
iterations, and we offer our API as a contribution.
% ``in its own right''  -- there's an echo in here...

\item
Because the client
can perform very local reasoning (``@y@ is live before
@x:=y+2@''),%
\footnote
{Using \hoopl, it is not necessary to have the more complex rule
``if @x@~is live after @x:=y+2@ then @y@ is live before~it,''
because if @x@~is \emph{not} live after @x:=y+2@, the assignment
@x:=y+2@ will be eliminated.}
 analyses and transformations built on \ourlib\ 
are small, simple, and easy to get right.
Moreover, \ourlib\ helps you write correct optimizations:
it~statically rules out transformations that violate invariants
of the control-flow graph (Sections \ref{sec:graph-rep} and \ref{sec:rewrites}),
and dynamically it can help find the first transformation that introduces a fault
in a test program (\secref{fuel}). 
\finalremark{SLPJ: I wanted to write more about open/closed,
but I like this sentence with its claim to both static and dynamic assistance,
and maybe the open/closed story is hard to understand here.}

% \item \ourlib{} makes use of GADTS and type functions to offer unusually
% strong static guarantees. In particular, nodes, basic blocks, and
% graphs are all statically typed by their open or closedness on entry, and
% their open or closedness on exit (\secref{graph-rep}). For example, an add instruction is
% open on entry and exit, while a branch is open on entry and closed on exit.
% Using these types we can statically guarantee that, say, an add instruction
% is rewritten to a graph that is also open on both entry and exit; and 
% that the user cannot construct a block where an add instruction follows an
% unconditional branch.  We know of no other system that offers 
% static guarantees this strong.

\item \ourlib{} implements subtle algorithms, including 
(a)~interleaved analysis and rewriting, (b)~speculative rewriting,
(c)~computing fixed points, and (d)~dynamic fault isolation.
Previous implementations of these algorithms---including three of our
own---are complicated and hard to understand, because the tricky pieces
are implemented all together, inseparably. 
A~significant contribution of this paper is 
a new way to structure the implementation so that each tricky piece
is handled in just 
one place, separate from all the others (\secref{engine}). 
%\remark{This is a very important claim---is
%it substantiated in \secref{engine}? And shouldn't the word
%\textbf{contribution} appear in this~\P?}
% \simon{Better?} % yes thanks ---NR
The result is sufficiently elegant that 
we emphasize the implementation as an object of interest in
its own right.
\end{itemize}
A working prototype of \ourlib{} is available from
\burl{http://ghc.cs.tufts.edu/hoopl} and also from Hackage.
It is no toy: an ancestor of this library is
part of the Glasgow Haskell Compiler, where it optimizes the
imperative {\PAL} code in GHC's back end.  The new design is far
nicer, and it will be in GHC shortly.

The API for \ourlib{} seems quite natural, but it requires
relatively sophisticated aspects of Haskell's type system, such
as higher-rank polymorphism, GADTs, and type functions.
As such, \ourlib{} offers a compelling case study in the utility
of these features.


\section{Dataflow analysis {\&} transformation by \texorpdfstring{\rlap{example}}{example}}
\seclabel{overview}
\seclabel{constant-propagation}
\seclabel{example:transforms}
\seclabel{example:xforms}

We begin by setting the scene, introducing some vocabulary, and
showing a small motivating example.
A control-flow graph, perhaps representing the body of a procedure,
is a collection of \emph{basic blocks}---or just ``blocks''.
Each block is a sequence of instructions,
beginning with a label and ending with a
control-transfer instruction that branches to other blocks.
% Each block has a label at the beginning,
% a sequence of 
%  -- each of which has a label at the 
% beginning.  Each block may branch to other blocks with arbitrarily
% complex control flow.
The goal of dataflow optimization is to compute valid
\emph{assertions} (or \emph{dataflow facts}), 
then use those assertions to justify code-improving
transformations (or \emph{rewrites}) on a \emph{control-flow graph}.  

Consider a concrete example: constant propagation with constant folding.
On the left we have a basic block; in the middle we have
facts that hold between statements (or \emph{nodes}) 
in the block; and at
the right we have the result of transforming the block 
based on the assertions:
\begin{verbatim}
      Before        Facts        After
          ------------{}-------------
      x := 3+4                   x := 7
          ----------{x=7}------------
      z := x>5                   z := True
          -------{x=7, z=True}-------
      if z                       goto L1
       then goto L1
       else goto L2
\end{verbatim}
Constant propagation works
from top to bottom.  We start with the empty fact.  
Given the empty fact and the node @x:=3+4@ can we make a (constant-folding)
transformation?
Yes!  We can replace the node with @x:=7@.
Now, given this transformed node,
and the original fact, what fact flows out of the bottom of
the transformed node?  
The~fact \{@x=7@\}.  
Given the fact \{@x=7@\} and the node @z:=x>5@, can we make a
transformation?  Yes: constant propagation can replace the node with @z:=7>5@.
Now, can we do another transformation?  Yes: constant folding can 
replace the node with @z:=True@.
And so the process continues to the end of the block, where we
can replace the conditional branch with an unconditional one, @goto L1@.

The example above is simple because the program has only straightline code;
when programs have loops, dataflow analysis gets more complicated.
For example, 
consider the following graph,
where we assume @L1@ is the entry point:
\begin{verbatim}
  L1: x=3; y=4; if z then goto L2 else goto L3
  L2: x=7; goto L3
  L3: ...
\end{verbatim}
Because control flows to @L3@ from two places,
we must \emph{join} the facts coming from those two places.
All paths to @L3@ produce the fact @y=@$4$,
so we can conclude that this fact holds at @L3@.
But depending on the the path to @L3@, @x@ may have different
values, so we conclude ``@x=@$\top$'',
meaning that there is no single value held by @x@ at @L3@.%
\footnote{
In this example @x@ really does vary at @L3@, but in general
the analysis might be conservative.}
The final result of joining the dataflow facts that flow to @L3@
is the new fact $@x=@\top \land @y=4@ \land @z=@\top$.

\seclabel{const-prop-example}

\seclabel{simple-tx}
\paragraph{Interleaved transformation and analysis.}
Our example \emph{interleaves} transformation and analysis.
Interleaving makes it far easier to write effective analyses.
If, instead, we \emph{first} analyzed the block
and \emph{then} transformed it, the analysis would have to ``predict''
the transformations.
For example, given the incoming fact \{@x=7@\}
and the instruction @z:=x>5@,
a pure analysis could produce the outgoing fact
\{@x=7@, @z=True@\} by simplifying @x>5@ to @True@.
But the subsequent transformation must perform
\emph{exactly the same simplification} when it transforms the instruction to @z:=True@!
If instead we \emph{first} rewrite the node to @z:=True@, 
and \emph{then} apply the transfer function to the new node, 
the transfer function becomes laughably simple: it merely has to see if the
right hand side is a constant (you can see actual code in \secref{const-prop-client}).
The gain is even more compelling if there are a number of interacting 
analyses and/or transformations; for more substantial
examples, consult \citet{lerner-grove-chambers:2002}.

\paragraph{Forwards and backwards.}
Constant propagation works \emph{forwards}, and a fact is typically an
assertion about the program state (such as ``variable~@x@ holds value~@7@'').  
Some useful analyses work \emph{backwards}.
A~prime example is live-variable analysis, where a fact takes the~form
``variable @x@ is live'' and is an assertion about the
\emph{continuation} of a program point.  For example, the fact ``@x@~is
live'' at a program point P is an assertion that @x@ is used on some program
path starting at \mbox{P}.  % TeXbook, exercise 12.6
The accompanying transformation is called dead-code elimination;
if @x@~is not live, this transformation 
replaces the node @x:=e@ with a no-op.

% ----------------------------------------
\section{Representing control-flow graphs} \seclabel{graph-rep}

\ourlib{} is a library that makes it easy to define dataflow analyses,
and transformations driven by these analyses, on control-flow graphs.
Graphs are composed from smaller units, which we discuss from the
bottom up:\finalremark{something about replacement graphs?}
\begin{itemize}
\item A \emph{node} is defined by \ourlib's client;
\ourlib{} knows nothing about the representation of nodes (\secref{nodes}).
\item A basic \emph{block} is a sequence of nodes (\secref{blocks}).
\item A \emph{graph} is an arbitrarily complicated control-flow graph,
composed from basic blocks (\secref{graphs}).
\end{itemize}

\subsection{Shapes: Open and closed}

Nodes, blocks, and graphs share important properties in common.
In particular, each can be \emph{open or closed at entry}
and \emph{open or closed at exit}.  
An \emph{open} point is one at which control may implicitly ``fall through;''
to transfer control at a \emph{closed} point requires an explicit
control-transfer instruction to a named label.
For example,
\begin{itemize}
\item A shift-left instruction is open on entry (because control can fall into it
from the preceding instruction), and open on exit (because control falls through
to the next instruction).
\item An unconditional branch is open on entry, but closed on exit (because 
control cannot fall through to the next instruction).
\item A label is closed on entry (because in \ourlib{} we do not allow
control to fall through into a branch target), but open on exit.
\end{itemize}
% This taxonomy enables \ourlib{} to enforce invariants:
% only nodes closed at entry can be the targets of branches, and only nodes closed
% at exits can transfer control (see also \secref{edges}).
% As~a consequence, all control transfers originate at control-transfer
% instructions and terminated at labels; this invariant dramatically
% simplifies analysis and transformation. 
These examples concern nodes, but the same classification applies
to blocks and graphs.  For example the block
\begin{code}
   x:=7; y:=x+2; goto L
\end{code}
is open on entry and closed on exit.  
This is the block's \emph{shape}, which we often abbreviate
``open/closed;''
we may refer to an ``open/closed block.''

The shape of a thing determines that thing's control-flow properties.
In particular, whenever E is a node, block, or graph,
% : \simon{Removed the claim about a unique entry point.}
\begin{itemize}
\item
If E is open at the entry, it has a unique predecessor; 
if it is closed, it may have arbitrarily many predecessors---or none.
\item
If E is open at the exit, it has a unique successor; 
if it is closed, it may have arbitrarily many successors---or none.
\end{itemize}
%%%%    
%%%%    
%%%%    % \item Regardless of whether E is open or closed, 
%%%%    % it has a unique entry point where execution begins.
%%%%    \item If E is closed at exit, control leaves \emph{only}
%%%%    by explicit branches from closed-on-exit nodes.
%%%%    \item If E is open at exit, control \emph{may} leave E
%%%%    by ``falling through'' from a distinguished exit point.
%%%%    \remark{If E is a node or block, control \emph{only} leaves E by
%%%%    falling through, but this isn't so for a graph.  Example: a body of a
%%%%    loop contains a \texttt{break} statement} \simon{I don't understand.
%%%%    A break statement would have to be translated as a branch, no?
%%%%    Can you give a an example? I claim that control only leaves an 
%%%%    open graph by falling through.}
%%%%    \end{itemize}


\subsection{Nodes} \seclabel{nodes}

The primitive constituents of a \ourlib{} control-flow graph are
\emph{nodes}, which are defined by the client.
Typically, a node might represent a machine instruction, such as an
assignment, a call, or a conditional branch.  
But \ourlib{}'s graph representation is polymorphic in the node type,
so each client can define nodes as it likes.
Because they contain nodes defined by the client,
graphs can include arbitrary client-specified data, including
(say) C~statements, method calls in an object-oriented language, or
whatever.


\begin{figure}
\begin{fuzzcode}{0.98pt}
data `Node e x where
  Label      :: Label -> Node C O
  `Assign     :: Var   -> Expr -> Node O O
  `Store      :: Expr  -> Expr -> Node O O
  `Branch     :: Label -> Node O C
  `CondBranch :: Expr  -> Label -> Label -> Node O C
    -- ... more constructors ...
\end{fuzzcode}
\caption{A typical node type as it might be defined by a client} 
\figlabel{cmm-node}
\end{figure}

\ourlib{} knows \emph{at compile time} whether a node is open or
  closed at entry and exit:
the type of a node has kind @*->*->*@, where the two type parameters
are type-level flags, one for entry and one for exit.
Such a type parameter may be instantiated only with type @O@~(for
open) or type~@C@ (for closed).
As an example,
\figref{cmm-node} shows a typical node type as it might be written by
one of \ourlib's {clients}.
The type parameters are written @e@ and @x@, for
entry and exit respectively.  
The type is a generalized algebraic data type;
the syntax gives the type of each constructor.  
%%% \cite{peyton-jones:unification-based-gadts}.
For example, constructor @Label@
takes a @Label@ and returns a node of type @Node C O@, where
the~``@C@'' says ``closed at entry'' and the~``@O@'' says ``open at exit''.  
The types @Label@, @O@, and~@C@ are 
defined by \ourlib{} (\figref{graph}).  

Similarly, an @Assign@ node takes a variable and an expression, and
returns a @Node@ open at both entry and exit; the @Store@ node is
similar.  The types @`Var@ and @`Expr@ are private to the client, and
\ourlib{} knows nothing of them.  
Finally, the control-transfer nodes @Branch@ and @CondBranch@ are open at entry
and closed at exit.  

Nodes closed on entry are the only targets of control transfers;
nodes open on entry and exit never perform control transfers;
and nodes closed on exit always perform control transfers\footnote{%
To obey these invariants,
a node for
a conditional-branch instruction, which typically either transfers control
\emph{or} falls through, must be represented as a two-target
conditional branch, with the fall-through path in a separate block.  
This representation is standard \cite{appel:modern},
and it costs nothing in practice:
such code is easily sequentialized without superfluous branches.
%a late-stage code-layout pass can readily reconstruct efficient code.
}.
Because of the position each type of node occupies in a
basic block,
we~often call them \emph{first}, \emph{middle}, and \emph{last} nodes
respectively.

\subsection{Blocks} \seclabel{blocks}

\begin{figure}
\begin{fuzzcode}{0.98pt}
data `O   -- Open
data `C   -- Closed

data `Block n e x where
 `BFirst  :: n C O                      -> Block n C O
 `BMiddle :: n O O                      -> Block n O O
 `BLast   :: n O C                      -> Block n O C
 `BCat    :: Block n e O -> Block n O x -> Block n e x

data `Graph n e x where
  `GNil  :: Graph n O O
  `GUnit :: Block n O O -> Graph n O O
  `GMany :: MaybeO e (Block n O C) 
        -> LabelMap (Block n C C)
        -> MaybeO x (Block n C O)
        -> Graph n e x

data `MaybeO ^ex t where
  `JustO    :: t -> MaybeO O t
  `NothingO ::      MaybeO C t

newtype `Label = Label Int

class `Edges n where
  `entryLabel :: n C x -> Label
  `successors :: n e C -> [Label]
\end{fuzzcode}
\caption{The block and graph types defined by \ourlib} 
\figlabel{graph} \figlabel{edges}
\end{figure}

\ourlib\ combines the client's nodes into
blocks and graphs, which, unlike the nodes, are defined by \ourlib{}
 (\figref{graph}).
A~@Block@ is parameterized over the node type~@n@
as well as over the same flag types that make it open or closed at
entry and exit.

The @BFirst@, @BMiddle@, and @BLast@ constructors create one-node
blocks.  
Each of these constructors is polymorphic in the node's \emph{representation}
but monomorphic in its \emph{shape}.
An~earlier representation of blocks used a single constructor 
of type \mbox{@n e x -> Block n e x@}, polymorphic in a node's representation \emph{and}
shape.
The representation of blocks in \figref{graph} has more constructors, but it
uses the power of GADTs to ensure that the shape of every node is
known statically.
This property makes no difference to clients, but it significantly
simplifies the implementation of analysis and transformation in
\secref{monomorphic-shape-outcome}. 

The @BCat@ constructor concatenates blocks in sequence. 
It~makes sense to concatenate blocks only when control can fall
through from the first to the second; therefore, 
two blocks may be concatenated {only} if each block is open at
the point of concatenation.
This restriction is enforced by the type of @BCat@, whose first 
argument must be open on exit, and whose second argument must be open on entry.
It~is statically impossible, for example, to concatenate a @Branch@
immediately before an
@Assign@.  
Indeed, the @Block@ type statically guarantees that any
closed/closed @Block@---which compiler writers normally 
call a ``basic block''---consists of exactly one closed/open node
(such as @Label@ in \figref{cmm-node}), 
followed by zero or more open/open nodes (@Assign@ or @Store@), 
and terminated with exactly one 
open/closed node (@Branch@ or @CondBranch@).  
Using GADTs to enforce these invariants is one of 
\ourlib{}'s innovations.
% Also notice that the definition of @Block@ guarantees that every @Block@ 
% has at least one node.
% SPJ: I think we've agreed to stick with PNil.
% \remark{So what? Who cares?  Why is it important
% that no block is empty?  (And if we had an empty block, we could ditch
% @PNil@ from @PGraph@, which woudl be an improvement.)}

\subsection{Graphs} \seclabel{graphs}

\ourlib{} composes blocks into graphs, which are also defined 
in  \figref{graph}.
Like @Block@, the data type @Graph@ is parameterized over
both nodes @n@ and its open/closed shape (@e@ and @x@).
It has three constructors.  The first two
deal with the base cases of open/open graphs:
an empty graph is represented by @GNil@ while a single-block graph
is represented by @GUnit@.

More general graphs are represented by @GMany@, which has three
fields: an optional entry sequence, a body, and an optional exit
sequence.
\begin{itemize}
\item 
If the graph is open at the entry, it contains an entry sequence of type 
@Block n O C@.
We could represent this sequence as a value of type
@Maybe (Block n O C)@, but we can do better: 
a~value of @Maybe@ type requires a \emph{dynamic} test,
but we know \emph{statically}, at compile time, that the sequence is present if and only
if the graph is open at the entry.
We~express our compile-time knowledge by using the type
@MaybeO e (Block n O C)@, a type-indexed version of @Maybe@
which is also defined in \figref{graph}:
the type @MaybeO O a@ is isomorphic to~@a@, while 
the type @MaybeO C a@ is isomorphic to~@()@.
\item 
The body of the graph is a collection of  closed/closed blocks.  
To~facilitate traversal of the graph, we represent a body as a finite
map from label to block.
\item 
The exit sequence is dual to the entry sequence, and like the entry
sequence, its presence or absence is deducible from the static type of the graph.
\end{itemize}

We can splice graphs together nicely; the cost is logarithmic in the
number of closed/closed blocks.
Unlike blocks, two graphs may be spliced together
not only when they are both open
 at splice point but also
when they are both closed---and not in the other two cases:
\begin{smallfuzzcode}{10.8pt}
`gSplice :: Graph n e a -> Graph n a x -> Graph n e x
gSplice GNil g2 = g2
gSplice g1 GNil = g1

gSplice (GUnit ^b1) (GUnit ^b2) = GUnit (b1 `BCat` b2)

gSplice (GUnit b) (GMany (JustO e) bs x) 
  = GMany (JustO (b `BCat` e)) bs x

gSplice (GMany e ^bs (JustO x)) (GUnit b2) 
  = GMany e bs (JustO (x `BCat` b2))

gSplice (GMany e1 ^bs1 (JustO x1)) (GMany (JustO e2) ^bs2 x2)
  = GMany e1 (bs1 `mapUnion` (b `addBlock` bs2)) x2
  where b = x1 `BCat` e2

gSplice (GMany e1 bs1 NothingO) (GMany NothingO bs2 x2)
  = GMany e1 (bs1 `mapUnion` bs2) x2
\end{smallfuzzcode}
This definition illustrates the power of GADTs: the
pattern matching is exhaustive, and all the open/closed invariants are
statically checked.  For example, consider the second-last equation for @gSplice@.
Since the exit link of the first argument is @JustO x1@,
we know that type parameter~@a@ is~@O@, and hence the entry link of the second 
argument must be @JustO e2@.
Moreover, block~@x1@ must be
closed/open, and block~@e2@ must be open/closed.  
We can therefore concatenate them
with @BCat@ to produce a closed/closed block, which is
added to the body of the result.

We~have carefully crafted the types so that if @BCat@ and @BodyCat@
are considered as associative operators, 
every graph has a unique representation.\finalremark{So what? (Doumont)
Need some words about how this is part of making the API simple for
the client---I've added something to the end of the paragraph, but I'm
not particularly thrilled.}
%%  \simon{Well, you were the one who was so keen on a unique representation!
%%  And since we have one, I think tis useful to say so. Lastly, the representation of
%%  singleton blocks is not entirely obvious.}
%%%%    
%%%%    An empty open/open graph is represented
%%%%    by @GNil@, while a closed/closed one is @gNilCC@:
%%%%    \par {\small
%%%%    \begin{code}
%%%%      gNilCC :: Graph C C
%%%%      gNilCC = GMany NothingO BodyEmpty NothingO
%%%%    \end{code}}
%%%%    The representation of a @Graph@ consisting of a single block~@b@ 
%%%%    depends on the shape of~@b@:\remark{Does anyone care?}
%%%%    \par{\small
%%%%    \begin{code}
%%%%      gUnitOO :: Block O O -> Graph O O
%%%%      gUnitOC :: Block O C -> Graph O C
%%%%      gUnitCO :: Block O C -> Graph C O
%%%%      gUnitCC :: Block O C -> Graph C C
%%%%      gUnitOO b = GUnit b
%%%%      gUnitOC b = GMany (JustO b) BodyEmpty   NothingO
%%%%      gUnitCO b = GMany NothingO  BodyEmpty   (JustO b)
%%%%      gUnitCC b = GMany NothingO  (BodyUnit b) NothingO
%%%%    \end{code}}
%%%%    Multi-block graphs are similar.
%%%%    From these definitions
To guarantee uniqueness, @GUnit@ is restricted to open/open
blocks.
If~@GUnit@ were more polymorphic, there would be 
more than one way to represent some graphs, and it wouldn't be obvious
to a client which representation to choose---or if the choice made a difference.


\subsection{Labels and successors} \seclabel{edges}

If \ourlib{} knows nothing about nodes, how can it know where
a control transfer goes, or what is the @Label@ at the start of a block?
To~answer such questions,
the standard Haskell idiom is to define a type class whose methods
provide exactly the operations needed;
\ourlib's type class, called @Edges@, is given in \figref{edges}.
The @entryLabel@ method takes a first node (one closed on entry, \secref{nodes})
and returns its @Label@;
the @successors@ method takes a last node (closed on exit) and returns
the @Label@s to 
which it can transfer control.  
A~middle node, which is open at both entry and exit, cannot refer to
any @Label@s, 
so no corresponding interrogation function is needed.

A node type defined by a client must be an instance of @Edges@.
In~\figref{cmm-node},
the client's instance declaration for @Node@ would be
\begin{code}
instance Edges Node where
  entryLabel (Label l) = l
  successors (Branch b) = [b]
  successors (CondBranch e b1 b2) = [b1, b2]
\end{code}
Again, the pattern matching for both functions is exhaustive, and
the compiler statically checks this fact.  Here, @entryLabel@ 
cannot be applied to an @Assign@ or @Branch@ node,
and any attempt to define a case for @Assign@ or @Branch@ would result
in a type error.

While it is required for the client to provide this information about
nodes, it is very convenient for \ourlib\ to get the same information
about blocks.
For its own internal use,
\ourlib{} provides this instance declaration for the @Block@ type:
\begin{code}
instance Edges n => Edges (Block n) where
  entryLabel (BFirst n) = entryLabel n
  entryLabel (BCat b _) = entryLabel b
  successors (BLast  n) = successors n
  successors (BCat _ b) = successors b
\end{code}
Because the functions @entryLabel@ and @successors@ are used to track control
flow \emph{within} a graph, \ourlib\ does not need to ask for the
entry label or successors of a @Graph@ itself.
Indeed, @Graph@ \emph{cannot} be an instance of @Edges@, because even
if a @Graph@ is closed at the entry, it does not have a unique entry label.
%
% A slight infelicity is that we cannot make @Graph@ an instance of @Edges@,
% because a graph closed on entry has no unique label.  Fortunately, we never
% need @Graph@s to be in @Edges@.
%
% ``Never apologize.  Never confess to infelicity.'' ---SLPJ




\begin{table}
\centerline{%
\begin{tabular}{@{}>{\raggedright\arraybackslash}p{1.03in}>{\scshape}c>{\scshape
}
      c>{\raggedright\arraybackslash}p{1.29in}@{}}
&\multicolumn1{r}{\llap{\emph{Specified}}\hspace*{-0.3em}}&
\multicolumn1{l}{\hspace*{-0.4em}\rlap{\emph{Implemented}}}&\\
\multicolumn1{c}{\emph{Part of optimizer}}
&\multicolumn1{c}{\emph{by}}&
\multicolumn1{c}{\emph{by}}&
\multicolumn1{c}{\emph{How many}}%
\\[5pt]
Control-flow graphs& Us & Us & One \\
Nodes in a control-flow graph & You & You & One type per intermediate language \\[3pt]
Dataflow fact~$F$    & You & You & One type per logic \\
Lattice operations & Us & You & One set per logic \\[3pt]
Transfer functions & Us & You & One per analysis \\
Rewrite functions & Us & You & One per \rlap{transformation} \\[3pt]
Solve-and-rewrite functions & Us & Us & Two (forward, backward) \\
\end{tabular}%
}
\caption{Parts of an optimizer built with \ourlib}
\tablabel{parts}
\end{table}



\section {Using \ourlib{} to analyze and transform graphs} \seclabel{using-hoopl}
\seclabel{making-simple}
\seclabel{create-analysis}

Now that we have graphs, how do we optimize them?
\ourlib{} makes it  easy for a
client to build a new dataflow analysis and optimization.  
The client must supply the following pieces:
\begin{itemize}
\item \emph{A node type} (\secref{nodes}).  
\ourlib{} supplies the @Block@ and @Graph@ types
that let the client build control-flow graphs out of nodes.
\item \emph{A data type of facts} and some operations over 
those facts (\secref{facts}).
Each analysis uses facts that are specific to that particular analysis,
which \ourlib{} accommodates by being polymorphic in 
the fact type.   
\item \emph{A transfer function} that takes a node and returns a
\emph{fact transformer}, which takes a fact flowing into the node and
returns the transformed fact that flows out of the node (\secref{transfers}).  
\item \emph{A rewrite function} that takes a node and an input fact,
and which returns either @Nothing@
or @(Just g)@ where @g@~is a graph that should
replace the node.  
The ability to replace a \emph{node} by a \emph{graph} that may include
internal control flow is
crucial for many code-improving transformations.
We discuss the rewrite function
in Sections \ref{sec:rewrites} and \ref{sec:shallow-vs-deep}.
\end{itemize}
These requirements are summarized in \tabref{parts}.
Because facts, transfer functions, and rewrite functions work closely together,
we represent their combination
as a single record of type @FwdPass@ (\figref{api-types}).


\ifpagetuning\enlargethispage{\baselineskip}\fi

Given a node type~@n@ and a @FwdPass@,
a client can ask \ourlib{}\ to analyze and rewrite a 
graph.
\hoopl\ provides a fully polymorphic interface, but for purposes of
exposition, we present a function that is specialized to a
closed/closed graph:
\begin{code}
analyzeAndRewriteFwdBody
  :: ( FuelMonad m    -- Gensym and other state
     , Edges n )      -- Access to flow edges
  => FwdPass m n f    -- Lattice, transfer, rewrite
  -> [Label]          -- Entry point(s)
  -> Graph n C C      -- Input graph
  -> FactBase f       -- Input fact(s)
  -> m ( Graph n C C  -- Result graph
       , FactBase f ) -- ... and its facts
\end{code}
Given a @FwdPass@, the 
analyze-and-rewrite function transforms a graph into
an optimized graph.
As its type shows, this function
is polymorphic in the types of nodes~@n@ and facts~@f@;
these types are determined entirely by the client.
The type of the monad~@m@ is also supplied by the client, but it must
meet certain constraints laid out by \hoopl, as described in
\secref{fuel-monad}. 


\begin{figure}
\begin{code}
data `FwdPass m n f
  = FwdPass { `fp_lattice  :: DataflowLattice f
            , `fp_transfer :: FwdTransfer n f
            , `fp_rewrite  :: FwdRewrite m n f }

------- Lattice ----------
data `DataflowLattice a = DataflowLattice
 { `fact_bot  :: a
 , `fact_join :: OldFact a -> NewFact a 
             -> (ChangeFlag, a) }

data `ChangeFlag = `NoChange | `SomeChange
newtype `OldFact a = OldFact a
newtype `NewFact a = NewFact a

------- Transfers ----------
newtype `FwdTransfer n f      -- abstract type
`mkFTransfer' 
 :: (forall e x . n e x -> f -> Fact x f)
 -> FwdTransfer n f

------- Rewrites ----------
newtype `FwdRewrite m n f     -- abstract type
`mkFRewrite' 
 :: (forall e x . n e x -> f -> m (FwdRes m n f e x)) 
 -> FwdRewrite m n f

data `FwdRes m n f e x 
  = FwdRes (Graph n e x) (FwdRewrite m n f)
  | `NoFwdRes

------- Fact-like things -------
type family   `Fact x f :: *
type instance Fact O f = f
type instance Fact C f = FactBase f

------- FactBase -------
type `FactBase f = LabelMap f
 -- A finite mapping from Labels to facts f
\end{code}
\caption{\ourlib{} API data types}
  \figlabel{api-types}
  \figlabel{lattice-type} \figlabel{lattice}
  \figlabel{transfers}  \figlabel{rewrites}
\end{figure}
% omit mkFactBase :: [(Label, f)] -> FactBase f



As well as taking and returning a graph with its entry point(s), the
function also takes input facts (the @FactBase@) and produces output facts. 
A~@FactBase@ is simply a finite mapping from @Label@ to facts.
The
output @FactBase@ maps each @Label@ in the @Body@ to its fact; if
the @Label@ is not in the domain of the @FactBase@, its fact is the
bottom element of the lattice.
The point(s) at which control flow may enter the graph are supplied by
the caller; the input @FactBase@ supplies any
facts that hold at these points.
For example, in our constant-propagation example from \secref{const-prop-example},
if the graph
represents the body of a procedure 
with parameters $x,y,z$, we would map the entry @Label@ to a fact
$@x=@\top \land @y=@\top \land @z=@\top$, to specify that the procedure's
parameters are not known to be constants.

The client's model of how @analyzeAndRewriteFwd@ works is as follows:
\ourlib\ walks forward over each block in the graph.
At each node, \ourlib\ applies the
rewrite function to the node and the incoming fact.  If the rewrite
function returns @NoFwdRes@, the node is retained as part of the output
graph, the transfer function is used to compute the downstream fact,
and \ourlib\ moves on to the next node.
But if the rewrite function returns @(FwdRes g rw)@,
indicating that it wants to rewrite the node to the replacement graph~@g@, then
\ourlib\ recursively analyzes and rewrites~@g@, using the new rewrite
function~@rw@, before moving on to the next node. 
A~node following a rewritten node sees 
\emph{up-to-date} facts; that is, its input fact is computed by
analyzing the replacement graph.

In this section we flesh out the
\emph{interface} to @analyzeAndRewriteFwd@, leaving the implementation for
\secref{engine}.  
{\hfuzz=7.2pt\par}

\subsection{Dataflow lattices} \seclabel{lattices} \seclabel{facts}

For each analysis or transformation, the client must define a type
of dataflow facts.
A~dataflow fact often represents an assertion
about a program point,\footnote
{In \ourlib, a program point is simply an edge in a
control-flow graph.}
but in general, dataflow analysis establishes properties of \emph{paths}:
\begin{itemize}
\item An assertion about all paths \emph{to} a program point is established
by a \emph{forwards analysis}. For example the assertion ``$@x@=@3@$'' at point P 
claims that variable @x@ holds value @3@ at P, regardless of the
path by which P is reached.
\item An assertion about all paths \emph{from} a program point is 
established by a \emph{backwards analysis}. For example, the 
assertion ``@x@ is dead'' at point P claims that no path from P uses 
variable @x@.
\end{itemize}

A~set of dataflow facts must form a lattice, and \ourlib{} must know
(a)~the bottom element of the lattice and (b)~how to take 
the least upper bound (join) of two elements.
To ensure that analysis
terminates, it is enough if every fact has a finite number of
distinct facts above it, so that repeated joins
eventually reach a fixed point.

In practice, joins are computed at labels.
If~$f_{\mathit{old}}$ is the fact currently associated with the
label~$L$, 
and if a transfer function propagates a new fact~$f_{\mathit{new}}$
into the label~$L$, 
the dataflow engine replaces $f_{\mathit{old}}$ with
the join  $f_{\mathit{old}} \join f_{\mathit{new}}$.
Furthermore, the dataflow engine wants to know if
  $f_{\mathit{new}} \join f_{\mathit{old}} = f_{\mathit{old}}$,
because if not, the analysis has not reached a fixed point.

The bottom element and join operation of a lattice of facts of
type~@f@ are stored in a value of type @DataflowLattice f@
(\figref{lattice}). 
%%Such a value is part of the  @FwdPass n f@ that is passed to
%%@analyzeAndRewriteFwd@ above.
As noted in the previous paragraph, 
\ourlib{} needs to know when the result of a join is equal
to one of the arguments joined.  
Because this information can often be computed cheaply together
when the join, \ourlib\ does not
 require a separate equality test on facts (which might be
expensive).
Instead, \ourlib\ requires that @fact_join@ return a @ChangeFlag@ as
well as the least upper bound.
The @ChangeFlag@ should be @NoChange@ if
the result is the same as the old fact, and
@SomeChange@ if the result differs.  

To help clients create lattices and join functions,
\hoopl\ includes functions and constructors that extend a type~@a@
with top and bottom elements.
In this paper, we use only type @`WithTop@, which has value
constructors with these types:
\begin{code}
  `PElem :: a -> WithTop a
  `Top   ::      WithTop a
\end{code}
\hoopl\ also provides a @`Bot@ constructor and types @WithTop@,
@`WithBot@, and @`WithTopAndBot@.
All are defined using a single GADT,
so that value constructors @Top@ and @Bot@ may be used with any of the
types.

The real convenience of @WithTop@, @WithBot@, and @WithTopAndBot@ is
that \hoopl\ provides a combinator that lifts
a join function on~@a@ to a join function on
@WithTop a@, and similarly for the other types.
The lifting combinator ensures that joins involving top and bottom
elements not only obey the appropriate algebraic laws but also set the
@ChangeFlag@ properly.




\remark{Notes about @changeIf@?}

% A~possible representation of the facts needed to implement
% constant propagation is shown in \figref{const-prop}.
% A~fact
% is represented as a finite map from a variable to a value of type
% @Maybe HasConst@.%
% A~variable $x$ maps to @Nothing@ iff $x=\bot$;
% $x$~maps to @Just Top@ iff $x=\top$;
% and $x$~maps to $@Just@\, (@I@\, k)$ iff $x=k$ (and similarly for
% Boolean constants).\remark{Ugh}
% Any one procedure has only
% finitely many variables; only finitely many facts are computed at any
% program point; and in this lattice any one fact can increase at most
% twice.  These properties ensure that the dataflow engine will reach a
% fixed point.


\subsection{The transfer function} \seclabel{transfers}

A~forward transfer function is presented with the dataflow fact(s) on
the edge(s) coming 
into a node, and it computes dataflow fact(s) on the outgoing edge(s).
In a forward analysis, the dataflow engine starts with the fact at the
beginning of a block and applies the transfer function to successive 
nodes in that block until eventually the transfer function for the last node
computes the facts that are propagated to the block's successors.
For example, consider this graph, with entry at @L1@:
\begin{code}
  L1: x=3; goto L2
  L2: y=x+4; x=x-1; 
      if x>0 then goto L2 else return
\end{code}
A forward analysis starts with the bottom fact \{\} at every label.
Analyzing @L1@ propagates this fact forward, by applying the transfer 
function successively to the nodes
of @L1@, emerging with the fact \{@x=3@\} for @L2@.
This new fact is joined with the existing (bottom) fact for @L2@.
Now the analysis propagates @L2@'s fact forward, again using the transfer
function, this time emerging with a new fact \mbox{\{@x=2@, @y=7@\}} for @L2@.
Again, the new fact is joined with the existing fact for @L2@, and the process
is iterated until the facts for each label reach a fixed point.

But wait!  What is the \emph{type} of the transfer function?
If the node is open at exit, the transfer function produces a single fact.
But what if the node is \emph{closed} on exit?
In that case the transfer function must
produce a list of (@Label@,fact) pairs, one for each outgoing edge.  
%
\emph{So the type of the transfer function's result 
depends on the shape of the node's exit.}
Fortunately, this dependency can be expressed precisely, at compile
time, by Haskell's (recently added) 
\emph{indexed type families}.
The relevant part of \ourlib's interface is given in \figref{transfers}.
A~forward transfer function supplied by a client, 
of type (@FwdTransfer@ @n@ @f@), 
is typically a function polymorphic in @e@ and @x@.  
It takes a 
node of type \mbox{(@n@ @e@ @x@)} and a fact of type~@f@, and it produces an
outgoing ``fact-like thing'' of type (@Fact@ @x@ @f@).  The 
type constructor @Fact@
should be thought of as a type-level function; its signature is given in the
@type family@ declaration, while its definition is given by two @type instance@
declarations.  The first declaration says that the fact-like thing
coming out of a node
\emph{open} at the exit is just a fact~@f@. The second declaration says that 
the fact-like thing
coming out of a node
\emph{closed} at the exit is a mapping from @Label@ to facts.

We have ordered the arguments such that if
\begin{code}
  `transfer_fn :: forall e x . n e x -> f -> Fact x f
  `node        :: n e x
\end{code}
then @(transfer_fn node)@ is a fact transformer:
\begin{code}
  transfer_fn node :: f -> Fact x f
\end{code}

So much for the interface.
What about the implementation?
The way GADTs work is that the compiler uses the value constructor for
type \mbox{@n@ @e@ @x@} to determine whether @e@~and~@x@ are open or
closed.
But we want to write functions that are \emph{polymorphic} in the node
type~@n@!
Such functions include
\begin{itemize}
\item
A function that takes a pair of @FwdTransfer@s for facts @f@~and~@f'@,
and returns a single @FwdTransfer@ for the fact @(f, f')@
\item
A function that takes a @FwdTransfer@ and wraps it in logging code, so
an analysis can be debugged by watching the facts flow through the
nodes
\end{itemize}
Such functions may also be useful in \hoopl's \emph{clients}:
% may need
%functions that are polymorphic in the node type~@n@:
\begin{itemize}
\item
A dominator analysis in the style of
\citet{cooper-harvey-kennedy:simple-dominance} requires only the
functions in the \texttt{Edges} type class; 
we have written such an analysis using transfer functions that are 
polymorphic in~@n@.
\end{itemize}
Because the mapping from
value constructors to shape is different for each node type~@n@, transfer
 functions cannot be polymorphic in both
the representation and the shape of nodes.
 Our implementation therefore sacrifices polymorphism in shape:
internally, \hoopl\ represents
a~@FwdTransfer n f@ as a \emph{triple} of functions,
each polymorphic in~@n@ but monomorphic in shape:
\begin{code}
newtype FwdTransfer n f 
  = FwdTransfers ( n C O -> f -> f
                 , n O O -> f -> f
                 , n O C -> f -> FactBase f
                 )
\end{code}
Such triples can easily be composed and wrapped without requiring a
pattern match on the value constructor of an unknown node
type.\remark{Need to refer to this junk in the conclusion}
Working with triples is tedious, but only \hoopl\ itself is forced to
work with triples; each client, which knows the node type, may supply
a triple,
but it typically supplies a single function polymorphic in the shape
of the (known) node.  




\subsection{The rewrite function} 
 \seclabel{rewrites} 
 \seclabel{example-rewrites}

%%%%    \begin{figure}
%%%%    \begin{smallfuzzcode}{6.6pt}
%%%%    type `AGraph n e x 
%%%%      = [Label] -> (Graph n e x, [Label])
%%%%    
%%%%    `withLabels :: Int -> ([Label] -> AGraph n e x)
%%%%               -> AGraph n e x
%%%%    withLabels n ^fn = \ls -> fn (take n ^ls) (drop n ls)
%%%%    
%%%%    `mkIfThenElse :: Expr -> AGraph Node O O
%%%%                 -> AGraph Node O O -> AGraph Node O O
%%%%    mkIfThenElse p t e
%%%%      = withLabels 3 $ \[l1,l2,l3] ->
%%%%        gUnitOC (BUnit (CondBranch p l1 l2)) `gSplice` 
%%%%        mkLabel l1 `gSplice` t `gSplice` mkBranch l3 `gSplice`
%%%%        mkLabel l2 `gSplice` e `gSplice` mkBranch l3 `gSplice`
%%%%        mkLabel l3
%%%%    
%%%%    `mkLabel  l = gUnitCO (BUnit (Label l))          
%%%%    `mkBranch l = gUnitOC (BUnit (Branch l))
%%%%    `gUnitOC  b = GMany (JustO b) BodyEmpty   NothingO
%%%%    `gUnitCO  b = GMany NothingO  BodyEmpty   (JustO b)
%%%%    \end{smallfuzzcode}
%%%%    \caption{The \texttt{AGraph} type and example constructions} \figlabel{agraph}
%%%%    \end{figure}

We compute dataflow facts in order to enable code-improving
transformations.
In our constant-propagation example,
the dataflow facts may enable us
to simplify an expression by performing constant folding, or to 
turn a conditional branch into an unconditional one.
Similarly, a liveness analysis may allow us to 
replace a dead assignment with a no-op.

A @FwdPass@ therefore includes a \emph{rewriting function}, whose
type, @FwdRewrite@, is given in \figref{api-types}.
A rewriting function takes a node and a fact, and optionally returns\ldots what?
At first one might
expect that rewriting should return a new node, but that is not enough:
We~might want to remove a node by rewriting it to the empty graph,
or more ambitiously, we might want to replace a high-level operation
with a tree of conditional branches or a loop, which would entail
introducing new blocks with internal control flow.
In~general, a rewrite function must be able to return a
\emph{graph}. 

\ifpagetuning\enlargethispage{0.8\baselineskip}\fi

Concretely, a @FwdRewrite@ takes a node and a suitably shaped
fact, and returns either @NoFwdRes@, indicating that the node should
not be replaced,
or $m\;@(FwdRes@\;\ag\;\rw@)@$, indicating that the node should
be replaced with~\ag: the replacement graph.  
The result is monadic because
if the rewriter makes graphs containing blocks,
it may need fresh @Label@s, which are supplied by a monad.
A~client may use its own monad or may use a monad or monad transformer
supplied by \hoopl. 
\remark{Forward reference??}

%%%% \ifpagetuning\enlargethispage{0.8\baselineskip}\fi

The type of @FwdRewrite@ in \figref{api-types} guarantees
\emph{at compile time} that
the replacement graph $\ag$ has
the \emph{same} open/closed shape as the node being rewritten.
For example, a branch instruction can be replaced only by a graph 
closed at the exit.  Moreover, because only an open/open graph can be 
empty---look at the type of @GNil@ in \figref{graph}---the type 
of @FwdRewrite@ 
guarantees, at compile time, that no head of a block (closed/open)
or tail of a block (open/closed) can ever be deleted by being
rewritten to an empty graph.

\subsection{Shallow vs deep rewriting}
 \seclabel{shallow-vs-deep}

Once the rewrite has been performed, what then?  
The rewrite returns a replacement graph, 
which must itself be analyzed, and its nodes may be further rewritten.
We~call @analyzeAndRewriteFwd@ to process the replacement
graph---but what @FwdPass@ should we use?
There are two common situations:
\begin{itemize}
\item Sometimes we want to analyze and transform the replacement graph
with an unmodified @FwdPass@, further rewriting the replacement graph.
This procedure is
called \emph{deep rewriting}. 
When deep rewriting is used, the client's rewrite function must
ensure that the graphs it produces are not rewritten indefinitely
(\secref{correctness}). 
\item Sometimes we want to analyze \emph{but not further rewrite} the
replacement graph.  This procedure is called \emph{shallow rewriting}.
It~is easily implemented by using a modified @FwdPass@
whose rewriting function always returns @NoFwdRes@.
\end{itemize}
Deep rewriting is essential to achieve the full benefits of
interleaved analysis and transformation
\citep{lerner-grove-chambers:2002}.
But shallow rewriting can be vital as well; 
for example, a forward dataflow pass that inserts
a spill before a call must not rewrite the call again, lest it attempt
to insert infinitely many spills.

An~innovation of \hoopl\ is to build the choice of shallow or deep
rewriting into each rewrite function, 
an idea that is elegantly captured by the
@FwdRes@ type returned by a @FwdRewrite@ (\figref{api-types}).
The first component of the @FwdRes@ is the replacement graph, as discussed earlier.
The second component, $\rw$, is a 
\emph{new rewriting function} to use when recursively processing
the replacement graph. 
For shallow rewriting this new function is
the constant @Nothing@ function; for deep rewriting it is the original
rewriting function.


\subsection{Composing rewrite functions and dataflow passes} \seclabel{combinators}

By requiring each rewrite to return a new rewrite function,
\hoopl\ enables a variety of combinators over
rewrite functions. 
\remark{This whole subsection needs to be redone in light of the new
(triple-based) representation.  It's not pretty any more.}
For example, here is a function
that combines two rewriting functions in sequence:
\remark{This code must be improved}
\smallverbatiminput{comb1}
What a beautiful type @thenFwdRw@ has! 
\remark{And what an ugly implementation!  Implementations must go.}
It tries @rw1@, and if @rw1@
declines to rewrite, it behaves like @rw2@.  But if
@rw1@ rewrites, returning a new rewriter @rw1a@, then the overall call also
succeeds, returning a new rewrite function obtained by combining @rw1a@
with @rw2@.  (We cannot apply @rw1a@ or @rw2@ 
directly to the replacement graph~@g@, 
because @r1@~returns a graph and @rw2@~expects a node.)
The rewriter @noFwdRw@ is the identity of @thenFwdRw@.
Finally, @thenFwdRw@ can 
combine a deep-rewriting function and a shallow-rewriting function,
to produce a rewriting function that is a combination of deep and shallow.
%%This is something that the Lerner/Grove/Chambers framework could not do,
%%because there was a global shallow/deep flag.
%% Our shortsightedness; Lerner/Grove/Chambers is deep only ---NR


A shallow rewriting function can be made deep by iterating
it:\remark{Algebraic law wanted!}
\smallverbatiminput{iterf}
If we have shallow rewrites $A$~and~$B$ then we can build $AB$,
$A^*B$, $(AB)^*$, 
and so on: sequential composition is @thenFwdRw@ and the Kleene star
is @iterFwdRw@.\remark{Do we still believe this claim?}
\remark{We can't define @iterFwdRew@ in terms of @thenFwdRew@ because
the second argument to @thenFwdRew@ would have to be the constant
nothing function when applied but would have to be the original triple
when passed to @thenFwdRew@ as the second argument (in the recursive
call).}


The combinators above operate on rewrite functions that share a common
fact type and transfer function.
It~can also be useful to combine entire dataflow passes that use
different facts.
We~invite you to write one such combinator, with type
\begin{code}
  `pairFwd :: Monad m
          => FwdPass m n f1 
          -> FwdPass m n f2
          -> FwdPass m n (f1,f2)
\end{code}
The two passes run interleaved, not sequentially, and each may help
the other,
yielding better results than running $A$~and then~$B$ or $B$~and then~$A$
\citep{lerner-grove-chambers:2002}.
%%  call these passes. ``super-analyses;''
%%  in \hoopl, construction of super-analyses is
%%  particularly concrete.

\subsection{Example: Constant propagation and constant folding} 
  \seclabel{const-prop-client}

% omit Binop :: Operator -> Expr -> Expr -> Expr
% omit Add :: Operator


%%  \begin{figure}
%%  {\small\hfuzz=3pt
%%  \begin{code}
%%  -- Types and definition of the lattice
%%  data `HasConst = `Top | `B Bool | `I Integer
%%  type `ConstFact = Map.Map Var HasConst
%%  `constLattice = DataflowLattice
%%    { fact_bot    = Map.empty
%%    , fact_extend = stdMapJoin constFactAdd }
%%    where
%%      `constFactAdd ^old ^new = (c, j)
%%        where j = if new == old then new else Top
%%              c = if j == old then NoChange else SomeChange
%%  
%%  -------------------------------------------------------
%%  -- Analysis: variable has constant value
%%  `varHasConst :: FwdTransfer Node ConstFact
%%  varHasConst (Label l)       f = lookupFact f l
%%  varHasConst (Store _ _)         f = f
%%  varHasConst (Assign x (Bool b)) f = Map.insert x (B b) f
%%  varHasConst (Assign x (Int  i)) f = Map.insert x (I i) f
%%  varHasConst (Assign x _)        f = Map.insert x Top   f
%%  varHasConst (Branch l)          f = mkFactBase [(l, f)]
%%  varHasConst (CondBranch (Var x) ^tid ^fid) f
%%    = mkFactBase [(tid, Map.insert x (B True)  f),
%%                  (fid, Map.insert x (B False) f)]
%%  varHasConst (CondBranch _ tid fid) f 
%%    = mkFactBase [(tid, f), (fid, f)]
%%  
%%  -------------------------------------------------------
%%  -- Constant propagation
%%  `constProp :: FwdRewrite Node ConstFact
%%  constProp node ^facts
%%    = fmap toAGraph (mapE rewriteE node)
%%    where
%%      `rewriteE e (Var x)
%%        = case Map.lookup x facts of
%%            Just (B b) -> Just $ Bool b
%%            Just (I i) -> Just $ Int  i
%%            _          -> Nothing
%%      rewriteE e = Nothing
%%  
%%  -------------------------------------------------------
%%  -- Simplification ("constant folding")
%%  `simplify :: FwdRewrite Node f
%%  simplify (CondBranch (Bool b) t f) _
%%    = Just $ toAGraph $ Branch (if b then t else f)
%%  simplify node _ = fmap toAGraph (mapE s_exp node)
%%    where
%%      `s_exp (Binop Add (Int i1) (Int i2))
%%         = Just $ Int $ i1 + i2
%%      ...  -- more cases for constant folding
%%  
%%  -- Rewriting expressions
%%  `mapE :: (Expr    -> Maybe Expr) 
%%        -> Node e x -> Maybe (Node e x)
%%  mapE f (Label _) = Nothing
%%  mapE f (Assign x e)  = fmap (Assign x) $ f e
%%   ...  -- more cases for rewriting expressions
%%  
%%  -------------------------------------------------------
%%  -- Defining the forward dataflow pass
%%  `constPropPass = FwdPass
%%     { fp_lattice  = constLattice
%%     , fp_transfer = varHasConst
%%     , fp_rewrite  = constProp `thenFwdRw` simplify } 
%%  \end{code}}
%%  \caption{The client for constant propagation and constant folding} \figlabel{old-const-prop}
%%  \end{figure}
\begin{figure}
\smallverbatiminput{cprop}
\caption{The client for constant propagation and constant folding\break (extracted automatically from code distributed with Hoopl)}
\figlabel{const-prop}
\end{figure}


\figref{const-prop} shows client code for
constant propagation and constant folding.
For each variable at each point in a graph, the analysis concludes one of three facts:
the variable holds a constant value (Boolean or integer),
the variable might hold a non-constant value,
or nothing is known about what the variable holds.
We~represent these facts using a finite map from a variable to a
fact of type (@WithTop Lit@).
A variable with a constant value maps to @Just k@, where @k@ is the constant value;
a variable with a non-constant value maps to @Just Top@;
and a variable with an unknown value maps to @Nothing@ (it is not
in the domain of the finite map).

% \afterpage{\clearpage}

The definition of the lattice (@constLattice@) is straightforward.
The bottom element is an empty map (nothing is known about the contents of any variable).
We~use the @stdMapJoin@ function to lift the join operation
for a single variable (@constFactAdd@) up to the map containing facts
for all variables.

% omit stdMapJoin :: Ord k => JoinFun v -> JoinFun (Map.Map k v)

For the transfer function, @varHasConst@, 
there are two interesting kinds of nodes:
assignment and conditional branch.
In the first two cases for assignment, a variable gets a constant value,
so we produce a dataflow fact mapping the variable to its value.
In the third case for assignment, the variable gets a non-constant value,
so we produce a dataflow fact mapping the variable to @Top@.
The last interesting case is a conditional branch where the condition
is a variable.
If the conditional branch flows to the true successor,
the variable holds @True@, and similarly for the false successor.
We update the fact flowing to each successor accordingly.

We do not need to consider complicated cases such as
an assignment @x:=y@ where @y@ holds a constant value @k@.
Instead, we rely on the interleaving of transformation
and analysis to first transform the assignment to @x:=k@,
which is exactly what our simple transfer function expects.
As we mention in \secref{simple-tx},
interleaving makes it possible to write
the simplest imaginable transfer functions, without missing
opportunities to improve the code.

The rewrite function for constant propagation, @constProp@,
simply rewrites each use of a variable to its constant value.
We use the auxiliary function @mapE@
to apply @rewriteE@ to each use of a variable in each kind of node;
in turn, the @rewriteE@ function checks if the variable has a constant
value and makes the substitution.  We assume an auxiliary function
\begin{code}
  `toAGraph :: Node e x -> AGraph Node e x
\end{code}

\figref{const-prop} also gives a completely separate rewrite function 
to perform constant
folding, called @simplify@.  It rewrites a conditional branch on a
boolean constant to an unconditional branch, and
to find constant subexpressions, 
it runs @s_exp@
on every subexpression.
Function @simplify@ does not need to check whether a variable holds a
constant value; it relies on @constProp@ to have replaced the
variable by the constant.
Indeed, @simplify@ does not consult the
incoming fact at all, and hence is polymorphic in~@f@.



We have written two @FwdRewrite@ functions 
because they are independently useful.  But in this case we
want to apply \emph{both} of them,
so we compose them with @thenFwdRw@.
The composed rewrite functions, along with the lattice and the
transfer function,
go into @constPropPass@ (bottom of \figref{const-prop}).
To improve a particular graph, we pass @constPropPass@ and the graph
to
@analyzeAndRewriteFwd@.


\subsection{Throttling the dataflow engine using ``optimization fuel''}
\seclabel{vpoiso}
\seclabel{fuel}

Debugging an optimization can be tricky:
an optimization may rewrite hundreds of nodes,
and any of those rewrites could be incorrect.
To debug dataflow optimizations, we use Whalley's
\citeyearpar{whalley:isolation} powerful technique
to identify the first rewrite that 
transforms a program from working code to faulty code.

The key idea is to~limit the number of rewrites that
are performed while optimizing a graph.
In \hoopl, the limit is called
\emph{optimization fuel}:
each rewrite costs one unit of fuel,
and when the fuel is exhausted, no more rewrites are permitted.
Because each rewrite leaves the observable behavior of the
program unchanged, it is safe to stop rewriting at any point.
Given a program that fails when compiled with optimization,
a test infrastructure uses binary search on the amount of
optimization fuel, until
it finds that the program works correctly after $n-1$ rewrites but fails
after $n$~rewrites.
The $n$th rewrite is faulty.


You may have noticed that @analyzeAndRewriteFwd@
returns a value in the @FuelMonad@ (\secref{using-hoopl}).
The @`FuelMonad@ is a simple state monad maintaining the supply of unused
fuel.  It also holds a supply of fresh labels, which are used by the rewriter
for making new blocks; more precisely, 
\hoopl\ uses these labels to
take the @AGraph@ returned by a pass's rewrite
function (\figref{rewrites}) and convert it to a~@Graph@.


\subsection{Fixed points and speculative rewrites} \seclabel{fixpoints}

Are rewrites sound, especially when there are loops?
Many analyses compute a fixed point starting from unsound
``facts''; for example, a live-variable analysis starts from the
assumption that all variables are dead.  This means \emph{rewrites
performed before a fixed point is reached may be unsound, and their results
must be discarded}.  Each iteration of the fixed-point computation must
start afresh with the original graph.  


Although the rewrites may be unsound, \emph{they must be performed}
(speculatively, and possibly recursively), 
so that the facts downstream of the replacement graphs are as accurate
as possible.
For~example, consider this graph, with entry at @L1@:
\par{\small
\begin{code}
  L1: x=0; goto L2
  L2: x=x+1; if x==10 then goto L3 else goto L2
\end{code}}
The first traversal of block @L2@ starts with the unsound ``fact'' \{x=0\};
but analysis of the block propagates the new fact \{x=1\} to @L2@, which joins the
existing fact to get \{x=$\top$\}.
What if the predicate in the conditional branch were @x<10@ instead
of @x==10@?
Again the first iteration would begin with the tentative fact \{x=0\}.
Using that fact, we would rewrite the conditional branch to an unconditional
branch @goto L3@.  No new fact would propagate to @L2@, and we would
have successfully (and soundly) eliminated the loop.
This example is contrived, but it illustrates that 
for best results we should
\begin{itemize}
\item Perform the rewrites on every iteration.
\item Begin each new iteration with the original, virgin graph.
\end{itemize}
This sort of algorithm is hard to implement in an imperative setting, where rewrites
mutate a graph in place.
But  with an immutable graph, implementing the algorithm
is trivially easy: we simply revert to the original graph at the start
of each fixed-point iteration.

\subsection{Correctness} \seclabel{correctness}

Facts computed by @analyzeAndRewriteFwd@ depend on graphs produced by the rewrite
function, which in turn depend on facts computed by the transfer function.
How~do we know this algorithm is sound, or if it terminates?
A~proof requires a POPL paper
\cite{lerner-grove-chambers:2002}, but we can give some
intuition.

\hoopl\ requires that a client's functions meet
 these preconditions:
\begin{itemize}
\item 
The lattice must have no \emph{infinite ascending chains}; that is,
every sequence of calls to @fact_extend@ must eventually return @NoChange@.
\item 
The transfer function must be 
\emph{monotonic}: given a more informative fact in,
it should produce a more informative fact out.
\item 
The rewrite function must be \emph{sound}:
if it replaces a node @n@ by a replacement graph~@g@, then @g@~must be
observationally equivalent to~@n@ under the  
assumptions expressed by the incoming dataflow fact~@f@.
%%\footnote{We do not permit a transformation to change
%%  the @Label@ of a node. We have not found any optimizations
%%  that are prevented (or even affected) by this restriction.}
\item 
The rewrite function must be \emph{consistent} with the transfer function;
that is, \mbox{@`transfer n f@ $\sqsubseteq$ @transfer g f@}.
For example, if the analysis says that @x@ is dead before the node~@n@,
then it had better still be dead if @n@ is replaced by @g@.
\item 
To ensure termination, a transformation that uses deep rewriting
must not return replacement graphs which 
contain nodes that could be rewritten indefinitely.
\end{itemize}
Without the conditions on monotonicity and consistency,
our algorithm will terminate, 
but there is no guarantee that it will compute
a fixed point of the analysis.  And that in turn threatens the
soundness of rewrites based on possibly bogus ``facts''.

However, when the preconditions above are met,
\begin{itemize} 
\item
The algorithm terminates.  The fixed-point loop must terminate because the 
lattice has no infinite ascending chains. And the client is responsible
for avoiding infinite recursion when deep rewriting is used.
\item 
The algorithm is sound.  Why? Because if each rewrite is sound (in the sense given above), 
then applying a succession of rewrites is also sound.
Moreover, a~sound analysis of the replacement graph
may generate only dataflow facts that could have been
generated by a more complicated analysis of the original graph.
\end{itemize}

\finalremark{Doesn't the rewrite have to be have the following property:
for a forward analysis/transform, if (rewrite P s) = Just s',
then (transfer P s $\sqsubseteq$ transfer P s').
For backward: if (rewrite Q s) = Just s', then (transfer Q s' $\sqsubseteq$ transfer Q s).
Works for liveness.
``It works for liveness, so it must be true'' (NR).
If this is true, it's worth a QuickCheck property!
}%
\finalremark{Version 2, after further rumination.  Let's define
$\scriptstyle \mathit{rt}(f,s) = \mathit{transform}(f, \mathit{rewrite}(f,s))$.
 Then $\mathit{rt}$ should
be monotonic in~$f$.  We think this is true of liveness, but we are not sure
whether it's just a generally good idea, or whether it's actually a 
precondition for some (as yet unarticulated) property of \ourlib{} to hold.}%

%%%%    \simon{The rewrite functions must presumably satisfy
%%%%    some monotonicity property.  Something like: given a more informative
%%%%    fact, the rewrite function will rewrite a node to a more informative graph
%%%%    (in the fact lattice.).
%%%%    \textbf{NR}: actually the only obligation of the rewrite function is
%%%%    to preserve observable behavior.  There's no requirement that it be
%%%%    monotonic or indeed that it do anything useful.  It just has to
%%%%    preserve semantics (and be a pure function of course).
%%%%    \textbf{SLPJ} In that case I think I could cook up a program that
%%%%    would never reach a fixed point. Imagine a liveness analysis with a loop;
%%%%    x is initially unused anywhere.
%%%%    At some assignment node inside the loop, the rewriter behaves as follows: 
%%%%    if (and only if) x is dead downstream, 
%%%%    make it alive by rewriting the assignment to mention x.
%%%%    Now in each successive iteration x will go live/dead/live/dead etc.  I
%%%%    maintain my claim that rewrite functions must satisfy some
%%%%    monotonicity property.
%%%%    \textbf{JD}: in the example you cite, monotonicity of facts at labels
%%%%    means x cannot go live/dead/live/dead etc.  The only way we can think
%%%%    of not to terminate is infinite ``deep rewriting.''
%%%%    }




\section{\ourlib's implementation}
\seclabel{engine}
\seclabel{dfengine}

\secref{making-simple}
gives a client's-eye view of \hoopl, showing how to use 
it to create analyses and transformations.
\hoopl's interface is simple, but 
the \emph{implementation} of interleaved analysis and rewriting is 
quite complicated.  \citet{lerner-grove-chambers:2002} 
do not describe their implementation.  We have written
at least three previous implementations, all of which
were long and hard to understand, and only one of which
provided compile-time guarantees about open and closed shapes.
We are not confident that any of these implementations are correct.

In this paper we describe our new implementation.  It is short
(about a third of the size of our last attempt), elegant, and offers
strong static shape guarantees.  The whole thing is about 300~lines of
code, excluding comments; this count includes both forward and backward
dataflow analysis and transformation.

We describe the implementation of \emph{forward} 
analysis and transformation.
The implementations of backward analysis and transformation are
exactly analogous and are included in \hoopl.



\subsection{Overview}

{\hfuzz=0.6pt
We concentrate on implementing @analyzeAndRewriteFwd@, whose
type is in \secref{using-hoopl}.
Its implementation is built on the hierarchy of nodes, blocks, and graphs
described in \secref{graph-rep}.  For each thing in the hierarchy,
we develop a function of this type:
\begin{code}
type `ARF ^thing n
 = forall f e x. FwdPass n f
              -> thing e x -> Fact e f 
              -> FuelMonad (RG n e x, Fact x f)
\end{code}
An @ARF@ (short for ``analyze and rewrite forward'') is a combination of
a rewrite and transfer function.
}%
An @ARF@ takes a @FwdPass@, a @thing@ (a node, block, or graph),
and an input fact,
and it returns a rewritten graph of type @(RG n e x)@ of the same shape
as the @thing@, plus a suitably shaped output fact.
%
%Regardless of whether @thing@ is a node, block, or graph, the result
%is always a graph.
% 
% point is made adequately in the client section ---NR
The type~@RG@ is internal to \hoopl; it is not seen by any client.
We use it, not @Graph@, for two reasons:
\begin{itemize}
\item The client is often interested not only in the facts flowing
out of the graph (which are returned in the @Fact x f@), 
but also in the facts on the \emph{internal} blocks
of the graph. A~replacement graph of type @(RG n e x)@ is decorated with
these internal facts.
\item A @Graph@ has deliberately restrictive invariants; for example,
a @GMany@ with a @JustO@ is always open at exit (\figref{graph}).  It turns
out to be awkward to maintain these invariants \emph{during} rewriting,
but easy to restore them \emph{after} rewriting by ``normalizing'' an @RG@.
\end{itemize}
The information in an @RG@ is returned to the client by
the normalization function @normalizeBody@, which
splits an @RG@ into a @Body@ and its corresponding @FactBase@:
\begin{code}
`normalizeBody :: Edges n => RG n f C C 
              -> (Body n, FactBase f)
\end{code}
\begin{figure}
\hfuzz=0.98pt
\begin{code}
data `RG n f e x where
  `RGNil   :: RG n f a a
  `RGCatO  :: RG n f e O -> RG n f O x -> RG n f e x
  `RGCatC  :: RG n f e C -> RG n f C x -> RG n f e x
  `RGUnit  :: Fact e f  -> Block n e x -> RG n f e x
\end{code}
\caption{The data type \texttt{RG} of rewritten graphs} \figlabel{rg}
\end{figure}
The constructors of @RG@ are given in \figref{rg}.
The essential points are that constructor @RGUnit@ is polymorphic in
the shape of a block, @RGUnit@ carries a fact as
well as a block, and the concatenation constructors record the shapes
of the graphs at the point of concatenation.
\remark{Not too happy with calling this ``concatenation''}
(A~record of the shapes is needed so that when
@normalizeBody@ is presented with a block carried by @RGUnit@, it is
known whether the block is an entry sequence, an exit sequence, or a
basic block.)
\remark{
Within \hoopl,
the @RG@~type is a great convenience.  Mutter mutter:
it carries facts as well as blocks, and it frees the client's
rewrite functions from any obligation to respect the invariants of
type @Graph@---I'm not convinced.}


We exploit the type distinctions of nodes, @Block@, @Body@,
and @Graph@ to structure the code into several small pieces, each of which
can be understood independently.  Specifically, we define a layered set of
functions, each of which calls the previous one:
\begin{code}
 arfNode  :: Edges n => ARF n n
 arfBlock :: Edges n => ARF (Block n) n
 arfBody  :: Edges n
          => FwdPass n f -> Body n -> FactBase f
          -> FuelMonad (RG n f C C, FactBase f)
 arfGraph :: Edges n => ARF (Graph n) n
\end{code} 
\begin{itemize} 
\item 
The @arfNode@ function processes nodes (\secref{arf-node}).
It handles the subtleties of interleaved analysis and rewriting,
and it deals with fuel consumption.  It calls @arfGraph@ to analyze
and transform rewritten graphs.
\item 
Based on @arfNode@ it is extremely easy to write @arfBlock@, which lifts
the analysis and rewriting from nodes to blocks (\secref{arf-block}).

%%% \ifpagetuning\penalty -10000 \fi


\item
Using @arfBlock@ we define @arfBody@, which analyzes and rewrites a
@Body@: a~group of closed/closed blocks linked by arbitrary
control flow.
Because a @Body@ is
always closed/closed and does not take shape parameters, function
@arfBody@ is less polymorphic than the others; its type is what
would be obtained by expanding and specializing the definition of
@ARF@ for a @thing@ which is always closed/closed and is equivalent to
a @Body@.

Function @arfBody@ takes care of fixed points (\secref{arf-body}).
\item 
Based on @arfBody@ it is easy to write @arfGraph@ (\secref{arf-graph}).
\end{itemize}
Given these functions, writing the main analyzer is a simple
matter of matching the external API to the internal functions:
\begin{code}
  `analyzeAndRewriteFwd
     :: forall n f. Edges n
     => FwdPass n f -> Body n -> FactBase f
     -> FuelMonad (Body n, FactBase f)

  analyzeAndRewriteFwd pass ^body facts
    = do { (^rg, _) <- arfBody pass body facts
         ; return (normalizeBody rg) }
\end{code}
 
\subsection{From nodes to blocks} \seclabel{arf-block}
\seclabel{arf-graph}

We begin our explanation with the second task:
writing @arfBlock@, which analyzes and transforms blocks.
\begin{code}
`arfBlock :: Edges n => ARF (Block n) n
arfBlock pass (BUnit node) f 
  = arfNode pass node f
arfBlock pass (BCat b1 b2) f 
  = do { (g1,f1) <- arfBlock pass b1 f  
       ; (g2,f2) <- arfBlock pass b2 f1 
       ; return (g1 `RGCatO` g2, f2) }
\end{code}
The code is delightfully simple.
The @BUnit@ case is implemented by @arfNode@.
The @BCat@ case is implemented by recursively applying @arfBlock@ to the two
sub-blocks, threading the output fact from the first as the 
input to the second.  
Each recursive call produces a rewritten graph;
we concatenate them with @RGCatO@. 

Function @arfGraph@ is equally straightforward:
\begin{code}
`arfGraph :: Edges n => ARF (Graph n) n
arfGraph _    GNil        f = return (RGNil, f)
arfGraph pass (GUnit blk) f = arfBlock pass blk f
arfGraph pass (GMany NothingO body NothingO) f
  = do { (^body', ^fb) <- arfBody pass body f
       ; return (body', fb) }
arfGraph pass (GMany NothingO body (JustO ^exit)) f
  = do { (body', fb) <- arfBody  pass body f
       ; (^exit', ^fx) <- arfBlock pass exit fb
       ; return (body' `RGCatC` exit', fx) }
 --  ... two more equations for GMany ...
\end{code}
The pattern is the same as for @arfBlock@: thread
facts through the sequence, and concatenate the results.
Because the constructors of type~@RG@ are more polymorphic than those
of @Graph@, type~@RG@ can represent
graphs more simply than @Graph@; for example, each element of a
@GMany@ becomes a single @RG@ object, and these @RG@ objects are then 
concatenated to form a single result of type~@RG@.

%% \ifpagetuning\penalty -10000 \fi

\subsection{Analyzing and rewriting nodes} \seclabel{arf-node}

Although interleaving analysis with transformation is tricky, we have
succeeded in isolating the algorithm in just two functions,  
@arfNode@ and its backward analog, @`arbNode@:
\begin{fuzzcode}{10.5pt}
`arfNode :: Edges n => ARF n n
arfNode ^pass n f
 = do { ^mb_g <- withFuel (fp_rewrite pass n f)
      ; case mb_g of
          Nothing -> return (RGUnit f (BUnit n),
                             fp_transfer pass n f)
          Just (FwdRes ^ag ^rw) ->
            do { g <- graphOfAGraph ag
               ; let ^pass' = pass { fp_rewrite = rw }
               ; arfGraph pass' g f } }
\end{fuzzcode}
The code here is more complicated, 
but still admirably brief.  
Using the @fp_rewrite@ record selector (\figref{api-types}),
we~begin by extracting the
rewriting function from the @FwdPass@,
and we apply it to the node~@n@ 
and the  incoming fact~@f@.  

The resulting @Maybe@ is passed to @withFuel@, which 
deals with fuel accounting:
\begin{code}
  `withFuel :: Maybe a -> FuelMonad (Maybe a)
\end{code}
If @withFuel@'s argument is @Nothing@, \emph{or} if we have run out of
optimization fuel (\secref{fuel}), @withFuel@ returns @Nothing@.
Otherwise, @withFuel@ consumes one unit of fuel and returns its
% defn Fuel
argument (which will be a @Just@).  That is all we need say about fuel.

In the @Nothing@ case, no rewrite takes place---either because the rewrite function
didn't want one or because fuel is exhausted.
We~return a single-node
graph @(RGUnit f (BUnit n))@, decorated with its incoming fact.
We~also apply the transfer function @(fp_transfer pass)@ 
to the incoming fact to produce the outgoing fact.
(Like @fp_rewrite@, @fp_transfer@ is a record selector of @FwdPass@.)

In the @Just@ case, we receive a replacement
@AGraph@ @ag@ and a new rewrite function~@rw@.
We~convert @ag@ to a @Graph@, using
\par{\small
\begin{code}
`graphOfAGraph :: AGraph n e x -> FuelMonad (Graph n e x)
\end{code}}
and we analyze the resulting @Graph@ with @arfGraph@.  
This analysis uses @pass'@, which contains the original lattice and transfer
function from @pass@, together with the new rewrite function~@rg@.

And that's it!  If~the client wanted deep rewriting, it is
implemented by the call to @arfGraph@;
if the client wanted
shallow rewriting, the rewrite function will have returned
@noFwdRw@ as~@rw@, which is implanted in @pass'@
(\secref{shallow-vs-deep}).

\subsection{Fixed points} \seclabel{arf-body}

Lastly, @arfBody@ deals with the fixed-point calculation.
This part of the implementation is the only really tricky part, and it is
cleanly separated from everything else:
\par{\small
\begin{code}
arfBody  :: Edges n
         => FwdPass n f -> Body n -> FactBase f
         -> FuelMonad (RG n f C C, FactBase f)
`arfBody pass body ^fbase
  = fixpoint (fp_lattice pass) (arfBlock pass) fbase $
    forwardBlockList (factBaseLabels fbase) body
\end{code}}
Function @forwardBlockList@ takes a list of possible entry points and @Body@,
 and it
 returns a linear list of
blocks, sorted into an order that makes forward dataflow efficient:
\begin{code}
`forwardBlockList 
  :: Edges n => [Label]
  -> Body n -> [(Label,Block n C C)]
\end{code}
For
example, if the @Body@ starts at block~@L2@, and @L2@ 
branches to~@L1@, but not vice versa, then \hoopl\ will reach a fixed point
more quickly if we process @L2@ before~@L1@.  
To~find an efficient order, @forwardBlockList@ uses
the methods of the @Edges@ class---@entryLabel@ and @successors@---to
perform a reverse depth-first traversal of the control-flow graph.
%%
%%The @Edges@ type-class constraint on~@n@ propagates to all the
%%@`arfThing@ functions.
%%  paragraph carrying too much freight
%%
The order of the blocks does not affect the fixed point or any other
part of the answer; it affects only the number of iterations needed to
reach the fixed point.

How do we know what entry points to pass to @forwardBlockList@? 
We treat
any block with an entry in the in-flowing @FactBase@ as an entry point.
\finalremark{Why does this work?}
{\hfuzz=0.8pt \par}

The rest of the work is done by @fixpoint@, which is shared by
both forward and backward analyses:
\begin{code}
`fixpoint :: forall n f.
     Edges n
  => Bool     -- going Forward?
  -> DataflowLattice f
  -> (Block n C C -> FactBase f -> 
               FuelMonad (RG n f C C, FactBase f))
  -> FactBase f 
  -> [(Label, Block n C C)]
  -> FuelMonad (RG n f C C, FactBase f)
\end{code}
Except for the mysterious @Bool@ passed as the first argument,
the type signature tells the story.
The third argument is
a function that analyzes and rewrites a single block; 
@fixpoint@ applies that function successively to all the blocks,
which are passed as the fifth argument.\finalremark{For consistency with the transfer
functions, blocks should come before @FactBase@, even though this change will
ugly up the call site some.}
The @fixpoint@ function maintains a
 ``Current @FactBase@''
which grows monotonically:
the initial value of the Current @FactBase@ is the fourth argument to
@fixpoint@,
and the Current @FactBase@ is augmented with the new facts that flow
out of each @Block@ as it is analyzed.
The @fixpoint@ function
keeps analyzing blocks until the Current @FactBase@ reaches a fixed
point.  

The code for @fixpoint@ is a massive 70 lines long;
for completeness, it
appears in Appendix~\ref{app:fixpoint}.  
The~code is mostly straightforward, although we try to be a bit clever
about deciding when a new fact means that another iteration over the
blocks will be required.
There is one more subtle point worth mentioning, which we highlight by 
considering a forward analysis of this graph, where execution starts at~@L1@:
\begin{code}
  L1: x:=3; goto L4
  L2: x:=4; goto L4
  L4: if x>3 goto L2 else goto L5
\end{code}
Block @L2@ is unreachable. 
But if we \naively\ process all the blocks (say in 
order @L1@, @L4@, @L2@), then we will start with the bottom fact for @L2@, propagate
\{@x=4@\} to @L4@, where it will join with \{@x=3@\} to yield
\{@x=@$\top$\}.  
Given @x=@$\top$, the
conditional in @L4@ cannot be rewritten, and @L2@~seems reachable.  We have
lost a good optimization.

Our implementation solves this problem through a clever trick that is
safe only for a forward analysis;
@fixpoint@ analyzes a block only if the block is
reachable from an entry point.
This trick is not safe for a backward analysis, which
 is why
@fixpoint@ takes a @Bool@ as its first argument:
it must know if the analysis goes forward.

Although the trick can be implemented in just a couple of lines of
code, the reasoning behind it is quite subtle---exactly the sort of
thing that should be implemented once in \hoopl, so clients don't have
to worry about it.

\section {Related work} \seclabel{related}

While there is a vast body of literature on
dataflow analysis and optimization,
relatively little can be found on
the \emph{design} of optimizers, which is the topic of this paper.
We therefore focus on the foundations of dataflow analysis
and on the implementations of some comparable dataflow frameworks.

\paragraph{Foundations}

When transfer functions are monotone and lattices are finite in height,
iterative dataflow analysis converges to a fixed point
\cite{kam-ullman:global-iterative-analysis}. 
If~the lattice's join operation distributes over transfer
functions,
this fixed point is equivalent to a join-over-all-paths solution to
the recursive dataflow equations
\cite{kildall:unified-optimization}.\footnote
{Kildall uses meets, not joins.  
Lattice orientation is conventional, and conventions have changed.
We use Dana Scott's
orientation, in which higher elements carry more information.}
\citet{kam-ullman:monotone-flow-analysis} generalize to some
monotone functions.
Each~client of \hoopl\ must guarantee monotonicity.

\ifcutting
\citet{cousot:abstract-interpretation:1977}
\else
\citet{cousot:abstract-interpretation:1977,cousot:systematic-analysis-frameworks}
\fi
introduce abstract interpretation as a technique for developing
lattices for program analysis.
\citet{schmidt:data-flow-analysis-model-checking} shows that
an all-paths dataflow problem can be viewed as model checking an
abstract interpretation.

\citet{muchnick:compiler-implementation} 
presents many examples of both particular analyses and related
algorithms.


The soundness of interleaving analysis and transformation,
even when not all speculative transformations are performed on later
iterations, was shown by
\citet{lerner-grove-chambers:2002}.


\paragraph{Frameworks}
Most dataflow frameworks support only analysis, not transformation.
The framework computes a fixed point of transfer functions, and it is
up to the client of 
the framework to use that fixed point for transformation.
Omitting transformation makes it much easier to build frameworks,
and one can find a spectrum of designs.
We~describe  two representative
designs, then move on to the prior frameworks that support interleaved
analysis and transformation.

The CIL toolkit \cite{necula:cil:2002}\finalremark{No good citation
for same reason as Soot below ---JD}
provides an analysis-only framework for C~programs.
The framework is limited to one representation of control-flow graphs
and one representation of instructions, both of which are provided by
the framework.
The~API is complicated;
much of the complexity is needed to enable the client to
affect which instructions 
the analysis iterates over.


The Soot framework is designed for analysis of Java programs
\cite{hendren:soot:2000}.\finalremark{This citation is probably the
best for Soot in general, but there doesn't appear 
 to be any formal publication that actually details the dataflow
 framework part. ---JD}
While Soot's dataflow library supports only analysis, not
 transformation, we found much 
 to admire in its design.
Soot's library is abstracted over the representation of
the control-flow graph and the representation of instructions.
Soot's interface for defining lattice and analysis functions is
like our own, 
although because Soot is implemented in an imperative style, 
additional functions are needed to copy lattice elements.
Like CIL, Soot provides only analysis, not transformation.


\finalremark{FYI, LLVM has Pass Managers that try to control the order of passes,
  but I'll be darned if I can find anything that might be termed a dataflow framework.}

The Whirlwind compiler contains the dataflow framework implemented
by \citet{lerner-grove-chambers:2002}, who were the first to 
interleave analysis and transformation.
Their implementation is much like our early efforts:
it is a complicated mix of code that simultaneously manages interleaving,
deep rewriting, and fixed-point computation.
By~separating these tasks, 
our implementation simplifies the problem dramatically.
Whirlwind's implementation also suffers from the difficulty of
maintaining pointer invariants in a mutable representation of
control-flow graphs, a problem we have discussed elsewhere
\cite{ramsey-dias:applicative-flow-graph}. 

Because speculative transformation is difficult in an imperative setting,
Whirlwind's implementation is split into two phases.
The first phase runs the interleaved analyses and transformations
to compute the final dataflow facts and a representation of the transformations
that should be applied to the input graph.
The second phase executes the transformations.
In~\hoopl, because control-flow graphs are immutable, speculative transformations
can be applied immediately, and there is no need
for a phase distinction.

%%% % repetitious...
%%%
%%%   \ourlib\ also improves upon Whirlwind's dataflow framework by providing
%%%   new support for the optimization writer:
%%%   \begin{itemize}
%%%   \item Using static type guarantees, \hoopl\ rules out a whole
%%%     class of possible bugs: transformations that produced malformed
%%%     control-flow graphs.
%%%   \item Using dynamic testing,
%%%     we can isolate the rewrite that transforms a working program
%%%     into a faulty program,
%%%     using Whalley's  \citeyearpar{whalley:isolation} fault-isolation technique.
%%%   \end{itemize}

In previous work \cite{ramsey-dias:applicative-flow-graph}, we
described a zipper-based representation of control-flow 
graphs, stressing the advantages
of immutability.
Our new representation, described in \secref{graph-rep}, is a significant improvement:
\begin{itemize}
\item
We can concatenate nodes, blocks, and graphs in constant time.
%Previously, we had to resort to Hughes's
%\citeyearpar{hughes:lists-representation:article} technique, representing
%a graph as a function.
\item
We can do a backward analysis without having
to ``unzip'' (and allocate a copy of) each block.
\item
Using GADTs, we can represent a flow-graph
node using a single type, instead of the triple of first, middle, and
last types used in our earlier representation.
This change simplifies the interface significantly:
instead of providing three transfer functions and three rewrite
functions per pass---one for 
each type of node---a client of \hoopl\ provides only one transfer
function and one rewrite function per pass.
\item
Errors in concatenation are ruled out at
compile-compile time by Haskell's static
type system.
In~earlier implementations, such errors were not detected until
the compiler~ran, at which point we tried to compensate
for the errors---but
the compensation code harbored subtle faults,
which we discovered while developing a new back end
for the Glasgow Haskell Compiler.
\end{itemize}

The implementation of \ourlib\ is also much better than
our earlier implementations.
Not only is the code simpler conceptually,
but it is also shorter:
our new implementation is about a third as long
as the previous version, which is part of GHC, version 6.12.




\section{What we learned}

> Some numbers, I have used it nine times, and would need the general fold
 > once to define blockToNodeList (or CB* equivalent suggested by you).
 > (We are using it in GHC to
 >   - computing hash of the blocks from the nodes
 >   - finding the last node of a block
 >   - converting block to the old representation (2x)
 >   - computing interference graph
 >   - counting Area used by a block (2x)
 >   - counting stack high-water mark for a block
 >   - prettyprinting block)


type-parameter hell, newtype hell, typechekcing hell, instance hell,
triple hell



We have spent six years implementing and reimplementing frameworks for
dataflow analysis and transformation.
 This formidable design problem taught us
two kinds of lessons:
we learned some very specific lessons about representations and
algorithms for optimizing compilers,
and we were forcibly reminded of some very general, old lessons that are well
known not just to functional programmers, but to programmers
everywhere.



%%  \remark{Orphaned: but for transfer functions that
%%  approximate weakest preconditions or strongest postconditions,
%%  monotonicity falls out naturally.}
%%  
%%  
%%  In conclusion we offer the following lessons from the experience of designing
%%  and implementing \ourlib{}.
%%  \begin{itemize}
%%  \item 
%%  Although we have not stressed this point, there is a close connection
%%  between dataflow analyses and program logic:
%%  \begin{itemize}
%%  \item
%%  A forward dataflow analysis is represented by a predicate transformer
%%  that is related to \emph{strongest postconditions}
%%  \cite{floyd:meaning}.\footnote
%%  {In Floyd's paper the output of the predicate transformer is called
%%  the \emph{strongest verifiable consequent}, not the ``strongest
%%  postcondition.''} 
%%  \item
%%  A backward dataflow analysis is represented by a predicate transformer
%%  that is related to \emph{weakest preconditions} \cite{dijkstra:discipline}.
%%  \end{itemize}
%%  Logicians write down the predicate transformers for the primitive
%%  program fragments, and then use compositional rules to ``lift'' them 
%%  to a logic for whole programs.  In the same way \ourlib{} lets the client
%%  write simple predicate transformers,
%%  and local rewrites based on those assertions, and ``lifts'' them to entire
%%  function bodies with arbitrary control flow.

Our main goal for \hoopl\ was to combine three good ideas (interleaved
analysis and transformation, optimization fuel, and an applicative
control-flow graph) in a way that could easily be reused by many, many
compiler writers.
Reuse requires abstraction, and as is well known,
designing good abstractions is challenging. 
\hoopl's data types and the functions over those types have been
through \emph{dozens} of revisions.
As~we were refining our design, we~found it invaluable to operate in
two modes:
In the first mode, we designed, built, and used a framework as an
important component of a real compiler (first Quick~{\PAL}, then GHC).
In the second mode, we designed and built a standalone library, then
redesigned and rebuilt it, sometimes going through several significant
changes in a week.
Operating in the first mode---inside a live compiler---forced us to
make sure that no corners were cut, that we were solving a real
problem, and that we did not inadvertently
cripple some other part of the compiler.
Operating in the second mode---as a standalone library---enabled us to
iterate furiously, trying out many more ideas than would have
been possible in the first mode.
We~have learned that alternating between these two modes leads to a
better design than operating in either mode alone.

We were forcibly reminded of timeless truths:
that interfaces are more important than implementations, and that data
is more important than code.
These truths are reflected in this paper, in which
we
have given \hoopl's API three times as much space as \hoopl's implementation.

We were also reminded that Haskell's type system (polymorphism, GADTs,
higher-order functions, type classes, and so on) is a remarkably
effective 
language for thinking about data and code---and that
Haskell lacks a language of interfaces (like ML's signatures) that
would make it equally effective for thinking about APIs at a larger scale.
Still, as usual, the types were a remarkable aid to writing the code:
when we finally agreed on the types presented above, the
code almost wrote itself.  

Types are widely appreciated at ICFP, but  here are three specific
examples of how types helped us:
\begin{itemize}
\item 
Reuse is enabled by representation-independence, which in a functional
language is
expressed through parametric polymorphism.
Making \ourlib{} polymorphic in the nodes 
made the code simpler, easier to understand, and easier to maintain.
In particular, it forced us to make explicit \emph{exactly} what
\ourlib\ must know about nodes, and to embody that knowledge in the
@Edges@ type class (\secref{edges}). 
\item
We are proud of using GADTs to 
track the open and closed shapes of nodes, blocks, and graphs at
compile time. 
Shapes may
seem like a small refinement, but they helped tremendously when
building \hoopl, and we expect them to help clients. 
%
% this paper is just not about run-time performance ---NR
%
%%%%    Moreover, the implementation is faster than it would otherwise be,
%%%%    because, say, a @(Fact O f)e@ is known to be just an @f@ rather than
%%%%    being a sum type that must be tested (with a statically known outcome!).
%
Giving the \emph{same} shapes
to nodes, blocks, and graphs helped our
thinking and helped to structure the implementation.
\item
In our earlier designs, graphs were parameterized over \emph{three} node
types: first, middle, and last nodes.
Those designs therefore required
three transfer functions, three rewrite functions, and so~on.
Moving to a single, ``shapely'' node type was a major breakthrough:
not only do we have just one node type, but our client need supply only
one transfer function and one rewrite function.
To~make this design work, however, we \emph{must} have
the type-level function for
@Fact@ (\figref{api-types}), to express how incoming
and outgoing facts depend on the shape of a node.
\end{itemize}

Dataflow optimization is usually described as a way to improve imperative
programs by mutating control-flow graphs.
Such transformations appear very different from the tree rewriting
that functional languages are so well known for, and that makes
functional languages so attractive for writing other parts of compilers.
But even though dataflow optimization looks very different from
what we are used to,
writing a dataflow optimizer
in a pure functional language was a huge win.
%  We could not possibly have conceived \ourlib{} in C++.
In~a pure functional language, not only do we know that
no data structure will be unexpectedly mutated,
but we are forced to be
explicit about every input and output, 
and we are encouraged to implement things compositionally.
This kind of thinking has helped us make
significant improvements to the already tricky work of Lerner, Grove,
and Chambers:
per-function control of shallow vs deep rewriting 
(\secref{shallow-vs-deep}),
combinators for dataflow passes (\secref{combinators}),
optimization fuel (\secref{fuel}),
and transparent management of unreachable blocks (\secref{arf-body}).
We~trust that these improvements are right only because they are
implemented in separate 
parts of the code that cannot interact except through
explicit function calls.
%%
%%An ancestor of \ourlib{} is in the Glasgow Haskell Compiler today,
%%in version~6.12.
With this new, improved design in hand, we are now moving back to
live-compiler mode,  pushing \hoopl\ into version
6.13 of the Glasgow Haskell Compiler.


\acks

The first and second authors were funded 
by a grant from Intel Corporation and
by NSF awards CCF-0838899 and CCF-0311482.
These authors also thank Microsoft Research Ltd, UK, for funding
extended visits to the third author.


\makeatother

\providecommand\includeftpref{\relax} %% total bafflement -- workaround
\IfFileExists{nrbib.tex}{\bibliography{cs,ramsey}}{\bibliography{dfopt}}
\bibliographystyle{plainnatx}


\clearpage

\appendix

% omit LabelSet :: *
% omit LabelMap :: *
% omit delFromFactBase :: FactBase f -> [(Label,f)] -> FactBase f
% omit elemFactBase :: Label -> FactBase f -> Bool
% omit elemLabelSet :: Label -> LabelSet -> Bool
% omit emptyLabelSet :: LabelSet
% omit factBaseLabels :: FactBase f -> [Label]
% omit extendFactBase :: FactBase f -> Label -> f -> FactBase f
% omit extendLabelSet :: LabelSet -> Label -> LabelSet
% omit getFuel :: FuelMonad Fuel
% omit setFuel :: Fuel -> FuelMonad ()
% omit lookupFact :: FactBase f -> Label -> Maybe f
% omit factBaseList :: FactBase f -> [(Label, f)]

\section{Code for \textmd{\texttt{fixpoint}}}
\label{app:fixpoint}

{\def\baselinestretch{0.95}\hfuzz=20pt
\begin{smallcode}
data `TxFactBase n f
  = `TxFB { `tfb_fbase :: FactBase f
         , `tfb_rg  :: RG n f C C -- Transformed blocks
         , `tfb_cha   :: ChangeFlag
         , `tfb_lbls  :: LabelSet }
 -- Set the tfb_cha flag iff 
 --   (a) the fact in tfb_fbase for or a block L changes
 --   (b) L is in tfb_lbls.
 -- The tfb_lbls are all Labels of the *original* 
 -- (not transformed) blocks

`updateFact :: DataflowLattice f -> LabelSet -> (Label, f)
           -> (ChangeFlag, FactBase f) 
           -> (ChangeFlag, FactBase f)
updateFact ^lat ^lbls (lbl, ^new_fact) (^cha, fbase)
  | NoChange <- ^cha2        = (cha,        fbase)
  | lbl `elemLabelSet` lbls = (SomeChange, new_fbase)
  | otherwise               = (cha,        new_fbase)
  where
    (cha2, ^res_fact) 
      = case lookupFact fbase lbl of
         Nothing -> (SomeChange, new_fact)
         Just ^old_fact -> fact_extend lat old_fact new_fact
    ^new_fbase = extendFactBase fbase lbl res_fact

fixpoint :: forall n f. Edges n
         => Bool        -- Going forwards?
         -> DataflowLattice f
         -> (Block n C C -> FactBase f
              -> FuelMonad (RG n f C C, FactBase f))
         -> FactBase f -> [(Label, Block n C C)]
         -> FuelMonad (RG n f C C, FactBase f)
fixpoint ^is_fwd lat ^do_block ^init_fbase ^blocks
 = do { ^fuel <- getFuel  
      ; ^tx_fb <- loop fuel init_fbase
      ; return (tfb_rg tx_fb, 
                tfb_fbase tx_fb `delFromFactBase` blocks) }
          -- The outgoing FactBase contains facts only for 
          -- Labels *not* in the blocks of the graph
 where
  `tx_blocks :: [(Label, Block n C C)] 
            -> TxFactBase n f -> FuelMonad (TxFactBase n f)
  tx_blocks []             tx_fb = return tx_fb
  tx_blocks ((lbl,blk):bs) tx_fb = tx_block lbl blk tx_fb
                                   >>= tx_blocks bs

  `tx_block :: Label -> Block n C C 
           -> TxFactBase n f -> FuelMonad (TxFactBase n f)
  tx_block ^lbl ^blk tx_fb@(TxFB { tfb_fbase = fbase
                               , tfb_lbls  = lbls
                               , tfb_rg    = ^blks
                               , tfb_cha   = cha })
    | is_fwd && not (lbl `elemFactBase` fbase)
    = return tx_fb    -- Note [Unreachable blocks]
    | otherwise
    = do { (rg, ^out_facts) <- do_block blk fbase
         ; let (^cha', ^fbase') 
                 = foldr (updateFact lat lbls) (cha,fbase) 
                         (factBaseList out_facts)
         ; return (TxFB { tfb_lbls = extendLabelSet lbls lbl
                        , tfb_rg   = rg `RGCatC` blks
                        , tfb_fbase = fbase'
                        , tfb_cha = cha' }) }

  loop :: Fuel -> FactBase f -> FuelMonad (TxFactBase n f)
  `loop fuel fbase 
    = do { let ^init_tx_fb = TxFB { tfb_fbase = fbase
                                 , tfb_cha   = NoChange
                                 , tfb_rg    = RGNil
                                 , tfb_lbls  = emptyLabelSet}
         ; tx_fb <- tx_blocks blocks init_tx_fb
         ; case tfb_cha tx_fb of
             NoChange   -> return tx_fb
             SomeChange -> setFuel fuel >>
                           loop fuel (tfb_fbase tx_fb) }
\end{smallcode}
\par
} % end \baselinestretch


\section{Index of defined identifiers}

This appendix lists every nontrivial identifier used in the body of
the paper.  
For each identifier, we list the page on which that identifier is
defined or discussed---or when appropriate, the figure (with line
number where possible).
For those few identifiers not defined or discussed in text, we give
the type signature and the page on which the identifier is first
referred to.

Some identifiers used in the text are defined in the Haskell Prelude;
for those readers less familiar with Haskell, these identifiers are
listed in Appendix~\ref{sec:prelude}.

\newcommand\dropit[3][]{}

\newcommand\hsprelude[2]{\noindent
  \texttt{#1} defined in the Haskell Prelude\\}
\let\hsprelude\dropit

\newcommand\hspagedef[3][]{\noindent
  \texttt{#2} defined on page~\pageref{#3}.\\}
\newcommand\omithspagedef[3][]{\noindent
  \texttt{#2} not shown (but see page~\pageref{#3}).\\}
\newcommand\omithsfigdef[3][]{\noindent
  \texttt{#2} not shown (but see Figure~\ref{#3} on page~\pageref{#3}).\\}
\newcommand\hsfigdef[3][]{%
  \noindent
  \ifx!#1!%
    \texttt{#2} defined in Figure~\ref{#3} on page~\pageref{#3}.\\
  \else
    \texttt{#2} defined on \lineref{#1} of Figure~\ref{#3} on page~\pageref{#3}.\\
  \fi
}    
\newcommand\hstabdef[3][]{%
  \noindent
  \ifx!#1!
    \texttt{#2} defined in Table~\ref{#3} on page~\pageref{#3}.\\
  \else
    \texttt{#2} defined on \lineref{#1} of Table~\ref{#3} on page~\pageref{#3}.\\
  \fi
}    
\newcommand\hspagedefll[3][]{\noindent
  \texttt{#2} {let}- or $\lambda$-bound on page~\pageref{#3}.\\}
\newcommand\hsfigdefll[3][]{%
  \noindent
  \ifx!#1!%
    \texttt{#2} {let}- or $\lambda$-bound in Figure~\ref{#3} on page~\pageref{#3}.\\
  \else
    \texttt{#2} {let}- or $\lambda$-bound on \lineref{#1} of Figure~\ref{#3} on page~\pageref{#3}.\\
  \fi
}    

\newcommand\nothspagedef[3][]{\notdefd\ndpage{#1}{#2}{#3}}
\newcommand\nothsfigdef[3][]{\notdefd\ndfig{#1}{#2}{#3}}
\newcommand\nothslinedef[3][]{\notdefd\ndline{#1}{#2}{#3}}

\newcommand\ndpage[3]{\texttt{#2}~(p\pageref{#3})}
\newcommand\ndfig[3]{\texttt{#2}~(Fig~\ref{#3},~p\pageref{#3})}
\newcommand\ndline[3]{%
  \ifx!#1!%
      \ndfig{#1}{#2}{#3}%
  \else
      \texttt{#2}~(Fig~\ref{#3}, line~\lineref{#1}, p\pageref{#3})%
  \fi
}

\newif\ifundefinedsection\undefinedsectionfalse

\newcommand\notdefd[4]{%
  \ifundefinedsection
    , #1{#2}{#3}{#4}%
  \else
    \undefinedsectiontrue
    \par
    \section{Undefined identifiers}
    #1{#2}{#3}{#4}%
  \fi
}

\begingroup
\raggedright

\input{defuse}%
\ifundefinedsection.\fi

\undefinedsectionfalse


\renewcommand\hsprelude[2]{\noindent
  \ifundefinedsection
    , \texttt{#1}%
  \else
    \undefinedsectiontrue
    \par
    \section{Identifiers defined in Haskell Prelude or a standard library}\label{sec:prelude}
    \texttt{#1}%
  \fi
}
\let\hspagedef\dropit
\let\omithspagedef\dropit
\let\omithsfigdef\dropit
\let\hsfigdef\dropit
\let\hstabdef\dropit
\let\hspagedefll\dropit
\let\hsfigdefll\dropit
\let\nothspagedef\dropit
\let\nothsfigdef\dropit
\let\nothslinedef\dropit

\input{defuse}
\ifundefinedsection.\fi



\endgroup


\iffalse

\section{Dataflow-engine functions}


\begin{figure*}
\setcounter{codeline}{0}
\begin{numberedcode}
\end{numberedcode}
\caption{The forward iterator}
\end{figure*}

\begin{figure*}
\setcounter{codeline}{0}
\begin{numberedcode}
\end{numberedcode}
\caption{The forward actualizer}
\end{figure*}


\fi



\end{document}




THE FUEL PROBLEM:


Here is the problem:

  A graph has an entry sequence, a body, and an exit sequence.
  Correctly computing facts on and flowing out of the body requires
  iteration; computation on the entry and exit sequences do not, since
  each is connected to the body by exactly one flow edge.

  The problem is to provide the correct fuel supply to the combined
  analysis/rewrite (iterator) functions, so that speculative rewriting
  is limited by the fuel supply.

  I will number iterations from 1 and name the fuel supplies as
  follows:

     f_pre      fuel remaining before analysis/rewriting starts
     f_0        fuel remaining after analysis/rewriting of the entry sequence
     f_i, i>0   fuel remaining after iteration i of the body
     f_post     fuel remaining after analysis/rewriting of the exit sequence

  The issue here is that only the last iteration of the body 'counts'.
  To formalize, I will name fuel consumed:

     C_pre      fuel consumed by speculative rewrites in entry sequence
     C_i        fuel consumed by speculative rewrites in iteration i of body
     C_post     fuel consumed by speculative rewrites in exit sequence

  These quantities should be related as follows:

     f_0    = f_pre - C_pref
     f_i    = f_0 - C_i            where i > 0
     f_post = f_n - C_post         where iteration converges after n steps

When the fuel supply is passed explicitly as parameter and result, it
is fairly easy to see how to keep reusing f_0 at every iteration, then
extract f_n for use before the exit sequence.  It is not obvious to me
how to do it cleanly using the fuel monad.


Norman
